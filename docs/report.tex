\documentclass[a4paper,11pt]{article}
\usepackage[margin=2cm]{geometry}

\usepackage[titletoc,toc,title,page]{appendix}
\usepackage[nodayofweek]{datetime}
\usepackage{cite}
\usepackage{graphicx}

% Date format.
\longdate

\usepackage{paralist}
\usepackage{mathcomp}
\usepackage{bm}
\usepackage{amsmath,amssymb,amsthm,enumitem}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{array,multirow}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\setcounter{secnumdepth}{4}

\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{minted}
\pagestyle{fancyplain}
\fancyhf{}
\lhead{\fancyplain{}{MSc Individual Project Report}}
\rhead{\fancyplain{}{\today}}
\cfoot{\fancyplain{}{\thepage}}

\newcommand{\reporttitle}{Implementation and Analysis of Integrated Information Applied to Models of Metastable Neural Populations}
\newcommand{\reportauthor}{Juan Carlos Farah}
\newcommand{\supervisor}{Professor Murray Shanahan\\Pedro Mediano}
\newcommand{\degreetype}{MSc Computing}

\begin{document}

% =======================================================================================
% TITLE PAGE
% =======================================================================================
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[width = 4cm]{./figures/imperial}\\[0.5cm]

\begin{center}

%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\Large Imperial College London}\\[0.5cm]
\textsc{\large Department of Computing}\\[0.5cm]

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries \reporttitle}\\
\HRule \\[1.5cm]

%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
\reportauthor
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
\supervisor
\end{flushright}
\end{minipage}\\[4cm]

%----------------------------------------------------------------------------------------
%	FOOTER & DATE SECTION
%----------------------------------------------------------------------------------------
\vfill % Fill the rest of the page with whitespace
Submitted in partial fulfillment of the requirements for the MSc degree in
\degreetype~of Imperial College London\\[0.5cm]

\makeatletter
\@date
\makeatother

\end{center}
\clearpage

% =======================================================================================
% INTENTIONAL BLANK PAGE
% =======================================================================================
\null
\clearpage

% =======================================================================================
% ABSTRACT
% =======================================================================================
\section{Abstract}
\label{sec:abstract}
Over the past ten years, the Integrated Information Theory of Consciousness (IIT) has arisen as a possible framework to explain the nature and properties of consciousness. The IIT claims that consciousness can be measured by a system's ability to integrate information, that is, to what extent information is generated by a system as a whole rather  than by the sum of its parts \cite{Tononi2003}. The IIT proposes integrated information ($\Phi$), as an appropriate quantifier for consciousness. Several versions of $\Phi$ have been proposed, each with slightly varying properties and applications. For this project we chose to implement empirical integrated information ($\Phi_E$) and its slight variation $\widetilde{\Phi}_E$ proposed by Seth and Barrett in \cite{Barrett2011}, as they are most suitable to the time series data outputted by the neural populations we considered.

Our implementation is written in Java and follows guidelines put forth by the Java Information Dynamics Toolkit (JIDT), which is widely used to study information dynamics \cite{Lizier2014}. The goal was to create an implementation that could be easily incorporated in the JIDT so as to facilitate its use by other researchers and information scientists. We then applied our implementations of $\Phi$ to neural models exhibiting metastable and chimera states following published architectures using Kuramoto \cite{Shanahan2010} and pyramidal inter-neuronal gamma (PING) oscillators \cite{Bhowmik2013}. We found that in the application to populations of Kuramoto oscillators, $\Phi_E$ and $\widetilde{\Phi}_E$ behaved consistently and exhibited correlations with other information theoretic measures presented in \cite{Shanahan2010}. Our application to communities of PING oscillators gave inconclusive results, which we hypothesise could be related to heuristic approximation methods used to overcome the complexity of calculating subsystem partitions and coalitions in such networks. This inconclusiveness and evidence of unexpected results in our surrogate data analysis provide us with extensions for this study as we integrate our implementation of $\Phi$ with the source code repository of the JIDT and make it available to the public. 

% =======================================================================================
% ACKNOWLEDGEMENTS
% =======================================================================================
\section{Acknowledgements}
\label{sec:ack}
I would like to thank my family, friends and fellow students for their ongoing support throughout this project. Many thanks to Dr David Bhowmik for taking the time to provide us with and explain in detail the source code used in \cite{Bhowmik2013}. I would also like to extend my regards to Professor Murray Shanahan for his guidance and supervision this summer. Finally, I would like to express my deepest gratitude to Pedro Mediano for his constant counsel and patience, his ongoing assistance with all aspects of this study, and for providing me with an excellent atmosphere for doing research. This project has been a team effort and I feel lucky to have such support from my colleagues at Imperial College. Thank you all for your help.
\clearpage

% =======================================================================================
% TABLE OF CONTENTS
% =======================================================================================
\tableofcontents
\clearpage

% =======================================================================================
% INTENTIONAL BLANK PAGE
% =======================================================================================
\null
\clearpage

% =======================================================================================
% INTRODUCTION
% =======================================================================================
\section{Introduction}
\label{sec:intro}
In the brain, metastability is defined as the ``simultaneous realisation of two competing tendencies: the tendency of the individual components to couple together and the tendency for the components to express their independent behaviour'' \cite{Kelso2012}. The Theory of Coordination Dynamics postulates that metastability is a key aspect of neurodynamics and it has been suggested that it plays an important role in several cognitive functions and consciousness \cite{Seth2009}.

By modelling synchronisation using spiking neural networks and large-scale approximations such as oscillators following the Kuramoto model explained in Section \ref{KuramotoModel}, this project aimed to investigate the properties of metastability and consciousness from an information-theoretic point of view. Specifically, we conducted a series of simulations to study the correspondence between several properties of these networks, some of which are highlighted in Section \ref{Measures}, with a particular focus on how they relate to integrated information. Our goal was to shed light on the dynamics and properties of information transmission and processing between populations of neurons as they oscillate in and out of transient coupled states.

This project had two clearly defined parts. The first was concerned with engineering an implementation of integrated information ($\Phi$), the measure proposed by the Integrated Information Theory of Consciousness (IIT) as an appropriate way to measure consciousness in a system \cite{Tononi2003}. The aim of this implementation was that it (1) was available to other researchers, (2) was easily maintainable and modular, and (3) followed design principles set forth by other available toolkits for information-theoretic analyses. Our particular implementation also had to be suitable for discrete time series data, as the systems to which we would apply it would provide output in that format.

The second part of this project concerned the application of this implementation of $\Phi$ to models of metastable neural populations. The two models we addressed were (1) populations of communities of Kuramoto oscillators as presented in \cite{Shanahan2010} and (2) populations of spiking neural networks following a pyramidal inter-neuronal gamma (PING) architecture as presented in \cite{Bhowmik2013}. Both of these models had been included in publications and documented as exhibiting metastable states, which we address in Section \ref{sec:bg:meta}. Our aim was to (1) study the performance of our implementation on these models and (2) study the relationship between our measure for integrated information and other information-theoretic measures computed for those systems. 

We structured this study in a way that could promote the future development of both parts separately and simultaneously. As explained in Section \ref{sec:impl}, our implementation is open source, readily available to the public and designed to accept external contributions to the code. We aim to make it available as a part of the widely used Java Information Dynamics Toolkit (JIDT) \cite{Lizier2014}, as proposed in Section \ref{sec:fw:jidt}. On the other hand, our application of this implementation of $\Phi$ on metastable neural populations is a strong proof of concept. It highlights various areas where further research could help us better understand the behaviour of $\Phi$ in different contexts and vis-\`{a}-vis other measures related to metastability in neural networks. Its application on other types of neuron and oscillator models as well as more rigorous analysis using surrogate data are only two of many extensions for which this project has laid the groundwork. These are considered in more detail in Section \ref{sec:fw}.

\clearpage

% =======================================================================================
% BACKGROUND
% =======================================================================================
\section{Background}
\label{sec:bg}
This project will focus on a number of concepts from the fields of Information Theory and Computational Neurodynamics. In order to provide a better understanding of the frameworks used in this study and introduce the structures and measures we included, we start this exposition with sections that provide a general explanation of these terms. We also expound on the basic pillars behind the Integrated Information Theory of Consciousness \cite{Tononi2008a}, which introduces the integrated information ($\Phi$) measure that constitutes the core analysis of this project. This is then followed by a brief discussion of the motivation behind our investigation, focusing on recent experimental studies that have been done in related fields and how our current project fits in that context.

% ==============
% KURAMOTO MODEL
% ==============
\subsection{Kuramoto Model}
\label{KuramotoModel}
Studies have shown that various regions of the brain display activity analogous to the behaviour modelled by coupled oscillators \cite{Bennett2004}. Oscillators are systems that generate a periodic signal, often resembling a sine or square wave \cite{Westra1999} of varying or determined frequencies. Coupled oscillators can transmit signals between each other and, as a unit, can exhibit complex motions that are often observed in nature \cite{Wolfs2004}. This motion can be interpreted as a flow of information or energy, and in the case of neural populations, electrical pulses transferred through synapses as neurons fire.

The Kuramoto model, proposed by Yoshiki Kuramoto, is a mathematical model for a system of coupled phase oscillators. It has widespread use in neuroscience as a framework for the study of synchronisation in dynamical systems as observed in nature \cite{Cumin2007}. The dynamics of this model are an extension of work by Winfree, which Kuramoto generalised to the form shown in Equation \ref{eq:kuramoto}, where $K \geq 0$ is the coupling strength, $N$ is the number of coupled phase oscillators $\theta_{i}(t)$ with natural frequencies $\omega_{i}$ distributed over probability density $g(\omega)$, which is assumed to be Gaussian \cite{Strogatz2000}.

\begin{equation} \label{eq:kuramoto}
\dot{\theta_i} = \omega_i + \frac{K}{N} \sum_{j=1}^{N} \sin(\theta_j - \theta_i), i = 1, ..., N
\end{equation}

% ===========================
% NETWORKS OF SPIKING NEURONS
% ===========================
\subsection{Networks of Spiking Neurons}
\label{sec:bg:snn}
As further explained in Section \ref{MSUSNN}, we modelled populations of spiking neurons in order to analyse the behaviour of our implementation in more biologically-plausible networks. The model that we used is based on work by Bhowmik and Shanahan \cite{Bhowmik2013}. Given that we based our implementation on the code presented in \cite{Bhowmik2013}, our simulations will closely follow the structure used in the original study. This includes the widely used Hodgkin-Huxley model \cite{Hodgkin1952} for the neurons in the network. As explained in \cite{Bhowmik2013}, Hodgkin and Huxley showed the presence of three types of ion currents. These consist of a sodium (Na+), a potassium (K+), and a predominantly chloride (Cl2) ion current, whose flow through the cell membrane depend on the level of voltage input.

The dynamics of the neurons in the network follow the Quadratic Integrate-and-Fire (QIF) model. The change in membrane potential over time of QIF neurons is given by the following equations, where $V$ is the membrane potential, $V_r$ is the resting potential, $V_t$ is the firing threshold, $C$ is the membrane capacitance, $\tau$ is the membrane time  constant, $I$ is the current and $R$ the resistance.

\begin{equation} \label{eq:qif}
\frac{dV}{dt} = \frac{1}{\tau}(V - V_ r)(V-V_t) +  \frac{I}{C}
\end{equation}

\begin{equation} \label{eq:tau}
\tau = RC
\end{equation}

Initially, as in \cite{Bhowmik2013} we set $\frac{1}{\tau} = 2$ and assume a membrane potential between $V_r = 265$ mV and $V_t = 245$ mV. As a future extension, it could be interesting to see the effects of fine-tuning these values on the results obtained after running our implementation of $\Phi$ on the network, as well as using different neuron models altogether, as proposed in Section \ref{sec:fw:snn}.

% =============
% METASTABILITY
% =============
\subsection{Metastability}
\label{sec:bg:meta}
Synchronisation in neural systems refers to a proximity in frequency or phase between two subsystems exhibiting a periodic oscillating behaviour \cite{Abarbanel1996}. In the case of populations of communities of oscillators, it can refer both to the concept of each community being internally synchronised, i.e. having similar frequencies and/or phases between the oscillators making up the community, and to equivalence between an overall measure of synchrony between the communities themselves.

It has been observed that ``periodic phenomena involving the synchronisation of multiple variables are prevalent both in nature and the human environment'' \cite{Shanahan2010} and these have been successfully modelled using systems of coupled oscillators such as those described in Section \ref{KuramotoModel}. Several studies have focused on investigating how these systems exhibit stable states of synchronisation \cite{Acebron2005}. However, increased synchronisation has also been linked to pathological behaviours such as epileptic seizures \cite{Arthuis2009}, which motivates the study of recurring, shorter bursts of synchronisation between oscillating structures that occur alongside periods of desynchronisation. This behaviour is defined as metastability. As defined generally by Shanahan, a system ``exhibits metastability if some or all of its members linger in the vicinity of a synchronised state without falling into such a state permanently'' \cite{Shanahan2010}. Formally, this phenomenon is ``quantified by the variance of synchrony within an individual oscillator cluster over time, averaged for all clusters in the system'' \cite{Bhowmik2013} and can be represented by a measure such as the metastability index defined in Equation \ref{eq:lambda}.

% ==============
% CHIMERA STATES
% ==============
\subsection{Chimera States}
\label{sec:bg:chimera}

As explained in \cite{Shanahan2010}, competition is a feature of many complex systems. In the brain, one of the ways in which competition manifests itself is in the presence of chimera states. Chimera states refer to the phenomena where a coalition of populations of neurons (or oscillators) are synchronised while other coalitions are desynchronised. This feature can be best explained visually by examining one of the plots presented in \cite{Shanahan2010}, which we have included in Figure \ref{Shanahan2010_Chimera}. A close examination of milliseconds 310 to 330 shows that the black, red, blue, green and purple communities are synchronised while the gray, turquoise and mustard communities are desynchronised. This is quantified by the pairwise synchrony measure, which gauges how synchronised a given pair of communities is.

\begin{figure}[H]
\centering
\includegraphics[scale = 0.5]{Shanahan2010_Chimera}
\caption{
	``Inter-community synchrony: Pairwise synchrony is plotted between one selected community (shown in black) and each of the eight communities (including itself). From time 310 to 330 the selected community is synchronised with several others, forming a temporary coalition.'' \cite{Shanahan2010}
	\label{Shanahan2010_Chimera}
}
\end{figure}

% ==============================================
% Integrated Information Theory of Consciousness
% ==============================================
\subsection{Integrated Information Theory of Consciousness}
In 2004, Tononi presented his Integrated Information Theory, postulating that consciousness could be defined as the capacity of a system to integrate information \cite{Tononi2004}. He claimed that there were two major problems posed by consciousness. The first concerns the issue of how to determine if and to what extent a system experiences consciousness, i.e. what properties can be measured to determine the consciousness of a given system. The second is related to the type of consciousness that a system may have, i.e. how to address the fact that different kinds of conscious experiences are associated to different parts of the brain and how damage to one of these areas can affect one type of sensory experience, but leave others unaffected (e.g. certain lesions only affect one's capability to perceive colours) \cite{Tononi2008a}.

We will focus on the former of these concerns. Tononi proposed that a good measure for consciousness would be integrated information ($\Phi$), which computes a system's ability to generate information as a whole, rather than as a sum of its parts. Although $\Phi$ yet remains to be recognised as an authoritative measure for consciousness, it has gained widespread attention both in the press and in the academic community \cite{Zimmer2010, Koch2008, Koch2009}. We thus decided to use this measure as the core of this project, with the aim of analysing its behaviour vis-\`{a}-vis the other measures presented in Section \ref{Measures} in the context of systems exhibiting metastable and chimera states. We further explain integrated information in Section \ref{II}.

% ========
% MEASURES
% ========
\subsection{Measures}
\label{Measures}
In this section we provide an overview of the different information-theoretic measures that we will use to analyse the populations of Kuramoto oscillators and spiking neural networks simulated in this study.

% -------
% Entropy
% -------
\subsubsection{Entropy ($H$)}
\label{sec:bg:entropy}

Entropy $H$ is a measure of uncertainty and for a discrete random variable $X$ that takes values in the space $\Omega_X$, can be calculated as shown in Equation \ref{eq:entropy}.

\begin{equation} \label{eq:entropy}
H(X) = - \sum_{x \in \Omega_X}P_X(x) \log_2 P_X(x)\\[2.5mm]
\end{equation}

% -------------------
% Conditional Entropy
% -------------------
\subsubsection{Conditional Entropy ($H$)}
\label{sec:bg:cond-entropy}

Known also as equivocation, the expected conditional entropy of $X$ given $Y$, where $Y$ is a discrete random variable that takes values in the space $\Omega_Y$, is denoted $H(X|Y)$ and calculated as shown in Equation \ref{eq:cond-entropy}.

\begin{equation}
\label{eq:cond-entropy}
H(X|Y) = \sum_{y \in \Omega_Y}H(X|Y=y)P_Y(y)
\end{equation}

% ------------------
% Mutual Information
% ------------------
\subsubsection{Mutual Information ($I$)}
\label{sec:bg:mi}

The mutual information $I(X,Y)$ between $X$ and $Y$ is the reduction in entropy of $X$ given that we know $Y$. It is calculated as shown in Equation \ref{eq:mi}.

\begin{equation}
\label{eq:mi}
I(X,Y) = H(X) - H(X|Y)
\end{equation}

% ---------------------------
% Kullback-Leibler Divergence
% ---------------------------
\subsubsection{Kullback-Leibler Divergence ($D_{KL}$)}
\label{sec:bg:kld}

The Kullback-Leibler divergence $D_{KL}(P || Q)$ quantifies the difference between two probability distributions $P$ and $Q$. It is a non-symmetric, non-negative measure that computes the information lost when Q is used to approximate P \cite{Burnham2002}. It is calculated as shown in Equation \ref{eq:kld}.

\begin{equation}
\label{eq:kld}
D_{KL}(P || Q) = \sum_{i} P(i) ln \frac{P(i)}{Q(i)}
\end{equation}

% ---------
% Synchrony
% ---------
\subsubsection{Synchrony ($\psi$)}
\label{sec:bg:sync}

In order to quantify how synchronised the oscillators within a given community are at a given time step $t$, you can use Equation \ref{eq:sync} to calculate $\psi_c(t)$\cite{Shanahan2010}. 

\begin{equation} \label{eq:sync}
\psi_c(t) = |\langle e^{i\theta_k(t)}\rangle_{k \in c}|
\end{equation}

Here, $\theta_k(t)$ refers to the phase of oscillator $k$ at time step $t$ and is averaged over all oscillators in community $c$. This produces a value that ranges from zero, when all the oscillators are completely desynchronised, to one, when they are all in synchrony \cite{Shanahan2010}.

% ----------------
% Global Synchrony
% ----------------
\subsubsection{Global Synchrony ($\Psi$)}
\label{sec:bg:global-sync}

Once you have calculated the synchrony of a community of oscillators $c$ for each time step $t$, you can average this for all of the time steps in a given simulation $T$ to obtain an aggregate measure of synchrony for that community ($\widehat{\psi}$), as shown in Equation \ref{eq:comm-sync}.  To obtain the global synchrony ($\Psi$), as given by Equation \ref{eq:global-sync}, you simply average $\widehat{\psi}$ over all the communities $c$ in a given population $C$ \cite{Shanahan2010}.

\begin{equation} \label{eq:comm-sync}
\widehat{\psi}_c = \frac{1}{|T|} \sum_{t \in T} \psi_c(t)
\end{equation}

\begin{equation} \label{eq:global-sync}
\Psi = \frac{1}{|C|} \sum_{c \in C} \widehat{\psi}_c
\end{equation}
 
% -------------------
% Metastability Index
% -------------------
\subsubsection{Metastability Index ($\lambda$)}
\label{sec:bg:lambda}

In \cite{Shanahan2010}, Shanahan proposes an index for metastability that builds on the synchrony measure described in Section \ref{sec:bg:sync}. We take $\psi_t$ for a given community $c$ over all time steps $t \in T$, where $T$ is the total number of time steps in the simulation, and then measure the variance over $T$, as denoted in Equation \ref{eq:sigma-met}. This measure can then be averaged over all communities $c$ in the system $C$ to produce an index of how metastable the population is overall. We call this measure $\lambda$ following the nomenclature established in \cite{Shanahan2010} as presented in Equation \ref{eq:lambda}.

\begin{equation} \label{eq:sigma-met}
\sigma_{\psi}(c) = \frac{1}{T - 1} \sum_{t \leq T }(\psi_{c}(t) - \langle \psi_{c}\rangle_T)^2
\end{equation}

\begin{equation} \label{eq:lambda}
\lambda = \langle \sigma_{\psi} \rangle_C
\end{equation}

% -------------
% Chimera Index
% -------------
\subsubsection{Chimera Index ($\chi$)}
\label{sec:bg:chi}

In order to calculate how chimera-like a system is at a given time step, we follow the measure proposed by Shanahan as a chimera index ($\chi$) \cite{Shanahan2010}. We take $\psi_c$, fix time and calculate its variance across communities. This indicates ``the level of spontaneous partitioning into synchronized and desynchronized subsets'' \cite{Bhowmik2013} and can be averaged as in \cite{Shanahan2010} to calculate $\chi$ shown below, where $C$ is the set of $M$ number of communities and $\psi_c(t)$ is the synchrony of community $c$ at time $t$ as described in Section \ref{sec:bg:sync}.

\begin{equation} \label{eq:var-sync}
\sigma_{chi}(t) = \frac{1}{M - 1}\sum_{c \in C}(\psi_c(t) - \langle \psi(t) \rangle_C)^2
\end{equation}

\begin{equation} \label{eq:chi}
\chi = \langle \sigma_{chi} \rangle_T
\end{equation}

% -----------------
% Coalition Entropy
% -----------------
\subsubsection{Coalition Entropy ($H_C$)} \label{sec:bg:hc}

As best described by Bhowmik and Shanahan, coalition entropy ($H_C$) ``measures the variety of metastable states entered by a system of oscillators and is calculated from the number of distinct states the system can generate and the probability of each state occurring'' \cite{Bhowmik2013}. We can calculate the coalition entropy $H_C$ for a system by computing ``how `mixed up' is the set of coalitions a system produces over a period of time'' \cite{Shanahan2010}. This is given by the equation below where ``$S$ is the set of distinct coalitions the system can generate and $p(s)$ is the probability of coalition $s$ arising in any given time point'' \cite{Shanahan2010}.

\begin{equation} \label{eq:hc}
H_C = - \frac{1}{\log_2 |S|}\sum_{s \in S}p(s) \log_2 (p(s))
\end{equation}

% ----------------
% Transfer Entropy
% ----------------
%\subsubsection{Transfer Entropy}
%Transfer entropy ``is an information theoretical measure that quantifies the statistical coherence between systems. It has the advantage that it does not only measure the coherence between two signals, but is able to distinguish between driving and responding elements and therefore between shared and transported information. This is called the directionality of the information flow.'' \cite{Buehlmann2010} Transfer entropy from process $J$ to process $I$ can be calculated using the equation below, where $l = k$ or $l = 1$ \cite{Schreiber2000}.
%
%\begin{equation} \label{eq:te}
%T_{J \rightarrow I} = \sum p(i_{n+1}, i_{n}^{(k)}, j_{n}^{(l)}) \log_2 \frac{p(i_{n+1} | i_{n}^{(k)}, j_{n}^{(l)})}{p(i_{n+1} | i_{n}^{(k)})}
%\end{equation}

% ---------------------
% Effective Information
% ---------------------
\subsubsection{Effective Information ($\phi$)}
\label{EI}

The effective information ($\phi$) of a system given a bipartition $\mathcal{B} = \lbrace P^1, P^2 \rbrace$ represents how much more information is generated by the whole system than by each member of $\mathcal{B}$. Specifically, if we want to calculate the effective information given the current state $X_t$ regarding the previous state $X_{t - \tau}$, with respect to bipartition $\mathcal{B}$, we sum the mutual information generated by each of the parts in $\mathcal{B}$ and subtract that from the mutual information generated by the whole system.

\begin{equation} \label{eq:ei}
\phi [X, \tau, \mathcal{B}] = I(X_{t-\tau}, X_t) - \sum_{k=1}^{2} I(P_{t-\tau}^k, P_{t}^k)
\end{equation}

% ----------------------
% Stochastic Interaction
% ----------------------
\subsubsection{Stochastic Interaction ($\tilde{\phi}$)}
\label{sec:bg:stochastic}

A variation of effective information is known as stochastic interaction ($\tilde{\phi}$), which is also known as \textit{expected} effective information \cite{Barrett2011}. It is defined as the average Kullback-Leibler divergence between the past state given the present state of system as a whole and the product of this for each subsystem given a set of partitions \cite{Ay2015}. For our purposes, as we will only be considering bipartitions, we can use the form presented by Barrett and Seth in \cite{Barrett2011}, as shown in Equation \ref{eq:stochastic}.

\begin{equation} \label{eq:stochastic}
\tilde{\phi} [X, \tau, \{M^1, M^2\}] = \sum_{k=1}^{2} H(M_{t-\tau}^k | M_{t}^k)  - H(X_{t-\tau} | X_t)
\end{equation}

% ----------------------
% Integrated Information
% ----------------------
\subsubsection{Integrated Information ($\Phi$)}
\label{II}

As noted in \cite{Barrett2011}, there are several definitions of integrated information. We will be using the empirical definition of integrated information ($\Phi_{E}$), as well as well as a slightly modified version, empirical integrated information tilde ($\widetilde{\Phi}_{E}$), that uses stochastic interaction ($\tilde{\phi}$) instead of effective information ($\phi$), as put forth by Barrett and Seth \cite{Barrett2011}.\\

\noindent\textbf{Empirical Integrated Information ($\Phi_{E}$)}\\[2.5mm]
\noindent Empirical integrated information ($\Phi_{E}$) is a version of integrated information that is best suited for time series data such as those that we will be using in our study. It is defined as the effective information of a system with respect to its minimum information bipartition.

\begin{equation}
\label{eq:ii}
\Phi [X, \tau] = \phi [X, \tau, \mathcal{B}^{MIB}(X, \tau)]
\end{equation}

\begin{equation}
\label{eq:mib}
\mathcal{B}^{MIB}(X, \tau) = \arg_{\mathcal{B}} \min \Big\lbrace \frac{\phi [X, \tau, \mathcal{B}]}{K(\mathcal{B})} \Big\rbrace
\end{equation}

\begin{equation}
\label{eq:norm}
K(\mathcal{B} = \lbrace P^1, P^2 \rbrace) = \min[H(P^1), H(P^2)]\\[2.5mm]
\end{equation}

\noindent\textbf{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$)}\\[2.5mm]
\noindent Empirical integrated information tilde ($\widetilde{\Phi}_{E}$) is a variation of $\Phi_{E}$ that uses stochastic interaction instead of effective information in order to determine the minimum information bipartition. Overall, $\widetilde{\Phi}_{E}$ behaves very similarly to $\Phi_{E}$, although it provides certain useful properties such as the fact that under certain assumptions it will never give a negative value.

\begin{equation}
\label{eq:ii-tilde}
\widetilde{\Phi} [X, \tau] = \tilde{\phi} [X, \tau, \widetilde{\mathcal{B}}^{MIB}(X, \tau)]
\end{equation}

\begin{equation}
\label{eq:mib-tilde}
\widetilde{\mathcal{B}}^{MIB}(X, \tau) = \arg_{\widetilde{\mathcal{B}}} \min \Big\lbrace \frac{\tilde{\phi} [X, \tau, \widetilde{\mathcal{B}}]}{\widetilde{K}(\widetilde{\mathcal{B}})} \Big\rbrace
\end{equation}

\begin{equation}
\label{eq:norm-tilde}
\widetilde{K}(\widetilde{\mathcal{B}} = \lbrace P^1, P^2 \rbrace) = \min[H(P^1), H(P^2)]
\end{equation}

% ==========
% MOTIVATION
% ==========
\subsection{Motivation}
\label{sec:bg:motivation}
One of the key objectives of this study is to investigate the link between certain information measures, with a focus on integrated information ($\Phi$) and metastability. Metastability, as discussed in Section \ref{sec:bg:meta}, has been observed in nature and more specifically in human brains. A study on patients recovering from pharmacologically induced comas found that as the subjects regained consciousness, neural activity exhibited discrete metastable states could be observed for periods lasting several minutes \cite{Hudson2014}. These findings underline the possible link between metastability and consciousness, highlighting the importance of further investigating this relationship. In doing so, researchers could potentially shed more light onto how the brain recovers from extended periods of unconsciousness.

On the other hand, a study conducted by Jean-R\'{e}mi King, Jacobo D. Sitt et al. showed a relationship between weighted symbolic mutual information (wSMI) and consciousness on patients recovering from states of coma \cite{King2013}. The wSMI is a measure of information introduced by the authors that ``evaluates the extent to which two [scalp electroencephalography] EEG signals present nonrandom joint fluctuations'' \cite{King2013}. Though a discussion of wSMI is outside the scope of this study, it is important to note that King, Sitt et al. were able to establish a link between an information-theoretic measure and consciousness. If it is possible to confirm such a correlation, then linking measures of information with metastability, both of which have been associated with consciousness, is a clear way to further this area of research and pave the way for future work. We consider these experimental findings as strong motivation for the present project.

% =======================================================================================
% IMPLEMENTATION
% =======================================================================================
\clearpage
\section{Implementation of Integrated Information ($\Phi$)}
\label{sec:impl}

In order to calculate the values of $\Phi$ for our sample populations, we implemented functions that compute empirical integrated information ($\Phi_{E}$) and empirical integrated information tilde ($\widetilde{\Phi}_{E}$), as described in Section \ref{II}. We decided that it would be best that these functions be available as part of the Java Information Dynamics Toolkit (JIDT) developed by Lizier and widely used to study the dynamics of complex systems \cite{Lizier2014}. Written completely in Java, our implementation follows the syntax, semantics and structure of other features in the JIDT, which is geared towards facilitating its adoption by its current users.

% ===========
% METHODOLOGY
% ===========
\subsection{Methodology}
\label{sec:impl:methods}

The structure put forth by the JIDT consists of defining a Java class that can calculate the measure in question. This class then exposes a series a methods that allow the user to bind the data that will be used for the calculation as well as perform any ancillary or preparatory operations required to compute the measure. Finally, the class will have a method to carry out the calculation and return the required output. Once an object of the calculator class is instantiated, it can be used for multiple calculations by binding new data to it and reinitialising the object's properties if needed. 

Another feature of the JIDT is that for most measures it supports, it defines a calculator both for the discrete and continuous versions of the measures, if applicable. For instance, it has a class to calculate entropy from continuous data called \texttt{EntropyCalculator} and its corresponding version for discrete data called \texttt{EntropyCalculatorDiscrete}. In some cases, it even defines a third version for mixed data, such as is the case for multivariate conditional mutual information. For our purposes, we are only interested in calculating the integrated information from discrete time series data and thus we have focused our efforts to implement a robust calculator for discrete data. Nevertheless, extending the implementation for continuous data would be useful in order to increment the breadth of the JIDT and is suggested as a possible extension to the project.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.5]{figures/architecture}
\caption{
	Architecture for the software included in this project.
	\label{fig:architecture}
}
\end{center}
\end{figure}

% ============
% ARCHITECTURE
% ============
\subsection{Architecture}

As mentioned in Section \ref{sec:impl:methods}, the main development language for the tools required by our project was Java. For all of our Java packages and integration with other libraries we managed the build process using Apache Maven, an open source tool that provides a uniform build system for Java projects \cite{TheApacheSoftwareFoundation}. This allowed us to include third-party libraries and manage release versions simply by editing a project object model (\texttt{pom.xml}) file, a snippet of which is included below.

\begin{minted}{xml}
<project xmlns="http://maven.apache.org/POM/4.0.0" ...>
	...
	<groupId>com.company</groupId>
	<artifactId>infotheoretic</artifactId>
	<version>1.0-SNAPSHOT</version>
	<packaging>jar</packaging>
	...
	<dependencies> 
		...
		<dependency>
			<groupId>org.mongodb</groupId>
			<artifactId>mongodb-driver</artifactId>
			<version>3.0.2</version>
		</dependency>
		<dependency>
			<groupId>com.google.guava</groupId>
			<artifactId>guava</artifactId>
			<version>18.0</version>
		</dependency>
	</dependencies>
</project>
\end{minted}

In addition to our implementation of $\Phi$ in Java for the JIDT, our project required a suite of tools and a supporting stack to facilitate the import, export and storage of data as well as a rigorous source version control system in order to facilitate integration with the JIDT. We thus decided to develop the utilities in Python and JavaScript, and use MongoDB \cite{MongoDB} as the database layer for the data inputted to and outputted from our simulations. The Python utilities also allowed us to import the Matlab data files outputted by third party codebases adapted from \cite{Shanahan2010} and \cite{Bhowmik2013}, which we ran in our simulations. Additionally, the Python utilities were used to fetch stored data, plot and export them as graphics for our analysis. This gave us the flexibility to be able to study the data generated by our simulations without having to run them every time. We used Git as our version control system, hosting our remote repository on \href{http://github.com}{GitHub} \cite{GitHub}, which will allow us to easily integrate with the future GitHub JIDT repository. The high level architecture of our implementation is depicted in Figure \ref{fig:architecture}.

% ================================
% EMPIRICAL INTEGRATED INFORMATION
% ================================
\subsection{Empirical Integrated Information}
\label{sec:impl:phi-e}

% -----------------------------------------
% Discrete Effective Information Calculator
% -----------------------------------------
\subsubsection{Discrete Effective Information Calculator}
\label{sec:impl:ei}

In order to calculate the empirical integrated information of a system, by definition you need to calculate its effective information. We implemented a class that computes the effective information of time series data given a time-step $\tau$, the number of input states for each variable in the input $base$, and a bipartition $\mathcal{B}$. As shown below, you initialise the calculator for a given base and $\tau$. You then bind the input data to the calculator object using its \texttt{addObservations} method.

\begin{minted}{java}
// Initialise effective information calculator.
EffectiveInformationCalculatorDiscrete eicd;
eicd = new EffectiveInformationCalculatorDiscrete(base, tau);
eicd.addObservations(input);
\end{minted}

As explained in Section \ref{EI} the effective information of a system given a bipartition $\mathcal{B}$ for a given time step $\tau$ represents the difference between the mutual information of the whole system given the current state $X_t$ regarding a previous state $X_{t-\tau}$ and the sum of the respective mutual information for each member of $\mathcal{B}$. In order to avoid calculating the mutual information of the system for each partition that is passed to the calculator, the \texttt{EffectiveInformationCalculatorDiscrete} class has a method \texttt{computeMutualInformationForSystem}, which only needs to run once for a given data set, as it stores the result in the \texttt{systemMutualInformation} property of the class.

At this point you are able to compute the effective information for a given bipartition with the method \texttt{computeForBipartition}. This method takes a partition as an argument in the form of an integer array specifying which variables to consider as one of the partitions of the system.

\begin{minted}{java}
// Calculate mutual information for the system.
double output = eicd.computeMutualInformationForSystem();

// Calculate effective information given a partition.
int[] partition = {0, 1, 2};
double output = eicd.computeForBipartition(partition);
\end{minted}

% Input
% -----
\noindent \textbf{Input}\\

\noindent The \texttt{EffectiveInformationCalculatorDiscrete} class takes advantage of JIDT's MutualInformationCalculatorDiscrete class in order to compute the mutual information of the system and each bipartition. However, this class does not expose a method to calculate the mutual information for two-dimensional inputs. Hence we created a special \texttt{Input} class with two main purposes.

\begin{enumerate}
\item{To reduce the time series input to one dimension.}
\item{To pair the input value at time step $t$ with that at $t-\tau$.}
\end{enumerate}

We achieve the first by taking advantage of JIDT's \texttt{MatrixUtils}. \texttt{MatrixUtils} exposes a method called \texttt{computeCombinedValues} which effectively reduces a two-dimensional array to one dimension. Because \texttt{computeCombinedValues} performs the reduction in a row-wise fashion and our time series input requires a column-wise reduction, we also take advantage of the \texttt{transpose} method in \texttt{MatrixUtils}. Additionally, we track the change in base, which will equal the original base to the power of the number of variables, as it is needed to calculate effective information. All this functionality is exposed in \texttt{Input}'s \texttt{reduce} method.

Once the \texttt{Input} object has been reduced, you can pair the inputs using the \texttt{pair} method. For this we simply take the reduced array $R$ of length $n$ and the time step $\tau$, and output an $n \times 2$ matrix $P$ where the first array contains the first $n - \tau$ elements of $R$ and the second array contains the last $n - \tau$ elements. Thus each column in $P$ contains the paired values required to calculate the mutual information and each row is passed onto the \texttt{MutualInformationCalculatorDiscrete} as shown below.

\begin{minted}{java}
// Calculate MI for whole system.
Input sys = new Input(data, base);
int sysBase = sys.getReducedBase();
int[][] sysPaired = sys.pair(tau);
micd = new MutualInformationCalculatorDiscrete(sysBase, 0);
micd.initialise();
micd.addObservations(sysPaired[0], sysPaired[1]);
double system = micd.computeAverageLocalOfObservations();
\end{minted}

% ------------------------------------------
% Discrete Integrated Information Calculator
% ------------------------------------------
\subsubsection{Discrete Integrated Information Calculator}
\label{sec:impl:ii:calculator}
By definition, the integrated information of a system is calculated by taking its effective information with respect to the minimum information bipartition, as explained in Section \ref{II}. Our implementation adds to the JIDT a class called \texttt{IntegratedInformationEmpiricalCalculatorDiscrete}. This calculator takes a time series input ($data$), the number of input states for each variable in the input ($base$) and a time-step ($\tau$) to find the minimum information bipartition and in turn calculate the integrated information of the system. As with the \texttt{EffectiveInformationCalculatorDiscrete}, we follow the patterns set forth by the JIDT, so an instance of this calculator is initialised with $base$ and $\tau$, and then $data$ is bound using the method \texttt{addObservations}.

\begin{minted}{java}
// Initialise integrated information calculator.
IntegratedInformationCalculatorDiscrete iicd;
iicd = new IntegratedInformationCalculatorDiscrete(base, tau);
iicd.addObservations(input);
\end{minted}

In order to find the possible partitions, the class has a method \texttt{computePossiblePartitions}, which takes advantage of JIDT's \texttt{MathUtils}' method \texttt{generateAllSets} to generate a set of all the possible partitions and store it in the calculator's \texttt{partitions} property.

Once the partitions have been calculated, it is possible to calculate the integrated information of the system using the method \texttt{compute}. This method will iterate through all of the possible bipartitions and calculate the effective information with respect to each.

\begin{minted}{java}
// Compute possible partitions and calculate integrated information.
iicd.computePossiblePartitions();
double output = iicd.compute();
\end{minted}

Note that as shown in Section \ref{II} and explained in \cite{Barrett2011}, the minimum information bipartition is calculated by finding the bipartition that minimises the effective information of the system given that bipartition divided by a normalisation factor. The normalisation factor is the minimum entropy of both parts of the bipartition. This is necessary because if a partition covers most of the system, it will provide almost as much information as the entirety of the network, which requires the normalisation factor to have a bias towards more equal partitions. Our implementation incorporates this through a method \texttt{computeNormalizationFactor}, which takes advantage of JIDT's reusable \texttt{EntropyCalculatorDiscrete} to calculate the partition with the minimum entropy.

\begin{minted}{java}
public double computeNormalizationFactor(int[][] part1, int[][] part2) {

	EntropyCalculatorDiscrete ecd = new EntropyCalculatorDiscrete(base);
	ecd.initialise();
	ecd.addObservations(part1);
	double entropy1 = ecd.computeAverageLocalOfObservations();

	ecd.initialise();
	ecd.addObservations(part2);
	double entropy2 = ecd.computeAverageLocalOfObservations();

	return Math.min(entropy1, entropy2);
}
\end{minted}

The normalisation factor ($k$) is then used inside the \texttt{compute} method to calculate and keep track of the minimum information bipartition ($MIB$). If the normalisation factor is zero, it means that one of the partitions has an entropy of zero, which means that this partition will not tell us anything about the rest of the system. To avoid division by zero, we return zero in those cases, otherwise we return the normalised effective information.

\begin{minted}{java}
	double k = computeNormalizationFactor(partition);
	double ei = eicd.computeForBipartition(partition);
	double normalizedEi = (k == 0) ? 0 : ei / k;
\end{minted}

Once the iteration is complete, the effective information with respect to the $MIB$ is then returned as the system's integrated information. 

% ======================================
% EMPIRICAL INTEGRATED INFORMATION TILDE
% ======================================
\subsection{Empirical Integrated Information Tilde}
\label{sec:impl:phi-e-tilde}

The second version of integrated information that we implemented was empirical integrated information tilde ($\widetilde{\Phi}_{E}$). As opposed to regular empirical integrated information, $\widetilde{\Phi}_{E}$ uses stochastic interaction instead of effective information when calculating the minimum information bipartition ($MIB$). Its advantages, as explained in Section \ref{II} include the fact that $\widetilde{\Phi}_{E}$ is always positive under certain assumptions.

% ------------------------------------------
% Discrete Stochastic Interaction Calculator
% ------------------------------------------
\subsubsection{Discrete Stochastic Interaction Calculator}
\label{sec:impl:stochastic}

In order to calculate the integrated information tilde of a system, by definition you need to calculate its stochastic interaction. We implemented a class that computes the stochastic interaction of time series data given a time-step $\tau$, the number of input states for each variable in the input $base$, and a bipartition $\mathcal{B}$. As shown below, you initialise the calculator for a given base and $\tau$. You then bind the input data to the calculator object using its \texttt{addObservations} method.

\begin{minted}{java}
// Initialise stochastic interaction calculator.
StochasticInteractionCalculatorDiscrete sicd;
sicd = new StochasticInteractionCalculatorDiscrete(base, tau);
sicd.addObservations(data);
\end{minted}

As noted in Section \ref{sec:bg:stochastic}, the stochastic interaction of a system given a bipartition $\mathcal{B}$ for a given time step $\tau$ represents the difference between the sum of the conditional entropy of each member of $B$ at state $X_{t-\tau}$ given state $X_t$ and the respective conditional entropy for system as a whole. We followed the same structure used for the \texttt{EffectiveInformationCalculatorDiscrete} class, so in order to avoid calculating the conditional entropy of the system for each partition that is passed to the calculator, the \texttt{StochasticInteractionCalculatorDiscrete} class has a method \texttt{computeConditionalEntropyForSystem}, which only needs to run once for a given data set, as it stores the result in the \texttt{systemConditionalEntropy} property of the class. As with the effective information calculator, we use the \texttt{Input} class in order to transform the input to the required form for our computations.

\subsubsection{Discrete Conditional Entropy Calculator}

Because stochastic interaction requires conditional entropy to be calculated both for the system as the whole and for each partition, we required an implementation of conditional entropy suited for the JIDT. We followed the pattern set by the \texttt{MutualInformationCalculatorDiscrete} class to define a \texttt{ConditionalEntropyCalculatorDiscrete} class that takes a base as a parameter, exposes an \texttt{addObservations} method to bind input data and a \texttt{compute} method to calculate the conditional entropy. Given that conditional entropy can be defined as shown in Equation \ref{eq:ce}, we take advantage of the already available \texttt{MutualInformationCalculatorDiscrete} and \texttt{EntropyCalculatorDiscrete} classes in our implementation for conditional entropy.

\begin{equation} \label{eq:ce}
H(X|Y) = H(X) - I(X; Y)
\end{equation}

\begin{minted}{java}
public double compute() {

	double mi = micd.computeAverageLocalOfObservations();
	double h = ecd.computeAverageLocalOfObservations();

	return h - mi;
}
\end{minted}

\subsubsection{Discrete Empirical Integrated Information Tilde Calculator}
\label{sec:impl:ii-tilde:calculator}

The implementation of the \texttt{IntegratedInformationEmpiricalTildeCalculatorDiscrete} class is identical to that of the \texttt{IntegratedInformationEmpiricalCalculatorDiscrete} class with the exception that the former uses the \texttt{StochasticInteractionCalculatorDiscrete} class instead of the \texttt{EffectiveInformationCalculatorDiscrete} class. Its usage is the same and it exposes the same methods, which make the two great candidates for having a common interface defined. The possibility of improving our implementation by refactoring the common code between these two classes is further explained in Section \ref{sec:fw:jidt}.

\subsection{Helper Methods}
Additionally, our implementation adds various auxiliary methods to the JIDT's helper classes, which can be also used by future developers of the toolkit.

\subsubsection{Coalition Entropy}
\label{sec:impl:hc}

As explained in Section \ref{sec:bg:hc}, coalition entropy ($H_c$) measures the diversity of states reached by a given network, calculated as shown in Equation \ref{eq:hc}. This measure is applied in \cite{Shanahan2010} on populations of communities of Kuramoto oscillators. Since the first application of our implementation of $\Phi$ was on data outputted using the model in \cite{Shanahan2010}, we wanted to ensure that the coalition entropy in our results was consistent with those of the original study. In order to confirm this, we implemented a coalition entropy calculator following the style of the JIDT. Being able to calculate coalition entropy also allowed us to investigate its relationship to integrated information. Its use is shown below.

\begin{minted}{java}
// Compute coalition entropy.
cecd = new CoalitionEntropyCalculatorDiscrete(base);
cecd.addObservations(obs);
double ce = cecd.compute();
\end{minted}

\subsubsection{Select All Rows Except}

Given that integrated information is deeply tied with the concept of partitions within a given network, our implementation required that we could easily extract information from our time series input data. Though the JIDT contained a method to select a given set of rows from a matrix, it did so only for two-dimensional double arrays. Hence we had to overload the method in order to support two-dimensional integer arrays. Additionally, we implemented a method in order to select all the rows in a matrix except a given set of them, whose indexes are specified as an array of integers. This method allows us to easily divide time series input data into bipartitions. 

\begin{minted}{java}
// Extracts all but the specified rows from the matrix
public static int[][] selectAllRowsExcept(int matrix[][], int rows[])
\end{minted}

\subsubsection{Contains}

Since we required a way of validating whether or not an integer was present in a given array we decided that it would be best to provide that functionality as a part of the JIDT. We thus implemented a lightweight method that takes an integer array and an integer, returning true if the integer is present in the array and false otherwise.

\begin{minted}{java}
// Returns true if an integer is contained in a array, false otherwise.
public static boolean contains(int[] array, int element)
\end{minted}

\subsubsection{Insert Vector Into Matrix}

We implemented a method to insert a vector into a matrix at a given column index, taking the vector input as an integer array. Although this implementation is specifically for integer vectors and matrices, it can be easily extended for other input types.

\begin{minted}{java}
public static void insertVectorIntoMatrix(int[] input, int[][] matrix, int column)
\end{minted}

This method is particularly useful for creating two-dimensional arrays that are then passed on to the methods that calculate the measures used in this project. Since our implementation stored data in a database that was then retrieved by iterating through a cursor, this helper method allowed us to fetch data and generate an input matrix without relying on other toolkits.

\begin{minted}{java}
// Obtain a cursor by passing a query to a MongoDB collection of documents (data).
MongoCursor<Document> cursor = data.find(eq("simulation_id", _id))
                                           .sort(ascending("_id"))
                                           .iterator();

// Insert array obtained from cursor as vector into matrix (obs).
int column = 0;
while (cursor.hasNext()) {
	Document d = cursor.next();
	ArrayList<Integer> array = (ArrayList) (d.get("data"));
	int[] vector = Ints.toArray(array);
	MatrixUtils.insertVectorIntoMatrix(vector, obs, column);
	column++;
}
\end{minted}

\subsubsection{Shuffle}
\label{sec:impl:shuffle}

In order to carry out surrogate data analysis for our project, we required methods that would be able to efficiently sort and shuffle the data generated by our simulations. Although Java provides a native \texttt{sort} method for arrays, there is no equivalent \texttt{shuffle} method. Our \texttt{shuffle} method implements Durstenfeld's version of the Fisher-Yates shuffle to provide a way of rearranging both the elements of a given array or the vectors in a two-dimensional matrix \cite{Durstenfeld1964}. The algorithm follows the procedure outlined below.

\begin{minted}{java}
Random rnd = new Random();
for (int i = array.length - 1; i > 0; i--) {

	// Get new index (between 0 and i + 1).
	int index = rnd.nextInt(i + 1);

	// Swap elements at given indexes.
	int a = array[index];
	array[index] = array[i];
	array[i] = a;
}
\end{minted}

Note that while the \texttt{shuffle} method for one-dimensional arrays does the shuffling in place, the method for two-dimensional inputs returns a new two-dimensional array of integers. Both methods have been implemented for integer arrays, but they could easily be extended to take other inputs. 

\begin{minted}{java}
// Shuffle array in place using Fisher Yates shuffle.
public static void shuffle(int[] array)

// Shuffle 2D matrix using Fisher Yates shuffle.
public static int[][] shuffle(int[][] matrix)
\end{minted}

\subsection{Data Sources and Data Store}

Given the computationally expensive nature of our simulations, a persistent store had to be used to save the output and results of each trial. For our data store we chose MongoDB, a documented-oriented database, as it provides a fast, scalable solution that does not require strict design decisions in advance. As we developed our tools, MongoDB's dynamic schemas allowed us to modify our objects without having to spend considerable time fixing compatibility issues. Additionally, MongoDB documents follow a JavaScript Object Notation (JSON)-like structure, which mirrors the structure of the Python objects we used when preparing the import data from Matlab data files for insertion to the database \cite{MongoDB}.

Additionally, the output from the populations of oscillators and spiking neurons, which we used as input to our integrated information calculator, were saved as Matlab files and stored directly on the filesystem. Using the Oct2Py Python Package \cite{Silvester}, which provides a Python interface to GNU Octave, an open source alternative to Matlab \cite{Eaton}, we were able to manipulate these and insert them as documents in our MongoDB instance.

\subsection{Utilities}
\label{sec:utils}
This project required a number of auxiliary tools to setup, run and analyse the data generated by the simulations to which we applied our implementation of $\Phi$. First we created utility scripts in Python that could import data into our MongoDB database. For instance, in order to input data from a Matlab or Octave file containing data outputted by a Kuramoto oscillator simulation, you simply instantiate an \texttt{OscillatorDataImporter} object, connect to the appropriate database and load the folder containing the files. Our implementation will by default import the data using multiple threads to provide different parameters for import. In the Kuramoto oscillator case, we define by default that there are five possible thresholds that define if two oscillators are synchronised or not. Hence the five import processes are run in parallel. Since MongoDB allows concurrent inserts into the same collection, this speeds up the import process significantly.

\begin{minted}{python}
data_folder = "path/to/folder"
db = "database_name"

odi = OscillatorDataImporter()
odi.connect(db)
odi.load(data_folder)
\end{minted}

After running a simulation, data is stored in MongoDB and can be retrieved for analysis and visualisation. For this purpose we defined a \texttt{DataPlotter} Python class. A \texttt{DataPlotter} object gets initialised with a database name, to which it connects. It then exposes a \texttt{plot} method, which can optionally take various arguments, such as a query defining what data to retrieve from the database, as well as a boolean, a file system path and a file extension to indicate if the graphics should be saved, where and in what format. By default, the graphics are not saved and all the simulations in the database are considered when plotting.

\begin{minted}{python}
query = {
	'duration': 5000,
	'num_oscillators': 8        
}

dp = DataPlotter(database="infotheoretic")
dp.plot(save=True,
        path="/path/to/my/plots",
        ext="svg",
        query=query)
\end{minted}


We also implemented a data generator in order to test our implementation using synthetic data as well as a JavaScript file that provides helper functions for MongoDB to manage the creation of indexes, rearranging of simulation data and sanity checks.

\subsection{Version Control}
Using Git as a source version control system allowed us to carefully track the evolution of our codebase. Currently, the JIDT is available through a repository on \href{https://code.google.com/}{Google Code}, which does not provide the functionality required for several contributors to seamlessly work actively on an open source project. Our extension for the JIDT is hosted on GitHub, which provides an intuitive web interface allow other users to clone and fork the repository for their own deployments. Additionally they can submit pull requests to contribute to the development of the JIDT.

% =======================================================================================
% APPLICATION TO POPULATIONS OF OSCILLATORS
% =======================================================================================
\clearpage
\section{Application to Populations of Oscillators}
\label{MSUKO}

Based on work by Shanahan \cite{Shanahan2010}, we used Kuramoto oscillators to create a series of systems of communities that modelled populations of neurons exhibiting metastability and chimera states. By running simulations on these networks, we were able to create discrete time series output that indicated which communities were synchronised at any give time step. These time series were inputted into our implementations of integrated information ($\Phi$) and used to compute various measures described in \ref{sec:app:osc:measures}. We were then able to analyse how these measures interacted with $\Phi$.

% ===========
% METHODOLOGY
% ===========
\subsection{Methodology}
\label{sec:osc:methods}
Our aim was to extend the work presented in \cite{Shanahan2010} and apply them to the measures proposed by Seth and Barrett in \cite{Barrett2011}, which we implemented as examined in detail in Section \ref{sec:impl}. In order to draw strong parallels between data extracted and analysed from our simulations and findings exposed in \cite{Shanahan2010}, we replicated, where possible, the methodology, methods and calculations used by Shanahan. Nevertheless, we structured our simulations and trials in a way that best fit our main purpose of analysing the behaviour of integrated information with regards to other information-theoretic measures.

% ---------
% The Model
% ---------
\subsubsection{The Model}
\label{sec:osc:mod}

In Shanahan \cite{Shanahan2010}, the model consists of eight communities of 32 Kuramoto oscillators each fully connected within their own communities and randomly connected to 32 other oscillators in the population. The phase of each oscillator is determined by Equation \ref{eq:shanahan:kuramoto}, which follows from the generalised Kuramoto model shown in Equation \ref{eq:kuramoto}. 

\begin{equation} \label{eq:shanahan:kuramoto}
\frac{d\theta_i}{dt} = \omega + \frac{1}{N + 1} \sum_{j=1}^{N} K_{ij} \sin(\theta_j - \theta_i - \alpha)
\end{equation}

We preserved the parameters as used by Shanahan in \cite{Shanahan2010}, where $\omega = 1$, $N = 63$, $A = u - v$ where $u + v = 1$ and $A = 0.2$. $K_{ij}$ is then defined depending on the connection between oscillators $i$ and $j$. $K_{ij} = u$ for intra-community connections and $K_{ij} = v$ for inter-community connections and $K_{ij} = 0$ when there is no connection present. Finally, $\beta = \frac{\pi}{2} - \alpha$, which determines the phase lag.

% ---------------
% The Simulations
% ---------------
\subsubsection{The Simulations}
\label{sec:osc:sims}

In order to generate the data required to apply our implementation of $\Phi$ to the populations of Kuramoto oscillators described in Section \ref{sec:osc:mod} we carried out our own simulations using the Matlab code from \cite{Shanahan2010} as a base. Each of our 7500 trials ran for 25 seconds with a value of $\beta$ assigned randomly from $0$ to $2\pi$. Every 5ms, the internal synchrony, as defined in Equation \ref{eq:sync}, was calculated and inserted into an $8 \times 8$ matrix representing the level of synchrony between all of the communities. Thus the output for every trial was a series of 5000 matrices, each of the form shown in Table \ref{tab:mat}.


% Synchrony Matrix
% ----------------
\begin{table}[H]
\centering

\begin{tabular}{c l | c c c c c c c c}

& & \multicolumn{8}{c}{Community \#} \\ [2mm]
& & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
\parbox[t]{2mm}{\multirow{8}{*}{\rotatebox[origin=c]{90}{Community \#}}}
& 1 & \textbf{0.29} & 0.41 & 0.16 & 0.44 & 0.13 & 0.41 & 0.51 & 0.43 \\
& 2 & 0.41 & \textbf{0.56} & 0.23 & 0.60 & 0.29 & 0.57 & 0.67 & 0.59 \\
& 3 & 0.16 & 0.23 & \textbf{0.20} & 0.30 & 0.14 & 0.23 & 0.34 & 0.26 \\
& 4 & 0.44 & 0.60 & 0.30 & \textbf{0.77} & 0.51 & 0.70 & 0.80 & 0.72 \\
& 5 & 0.13 & 0.29 & 0.14 & 0.51 & \textbf{0.35} & 0.41 & 0.51 & 0.43 \\
& 6 & 0.41 & 0.57 & 0.23 & 0.70 & 0.41 & \textbf{0.64} & 0.74 & 0.66 \\
& 7 & 0.51 & 0.67 & 0.34 & 0.80 & 0.51 & 0.74 & \textbf{0.84} & 0.76 \\
& 8 & 0.43 & 0.59 & 0.26 & 0.72 & 0.43 & 0.66 & 0.76 & \textbf{0.68} \\
\end{tabular}
\caption{Each trial outputted $5000$ $8 \times 8$ matrices displaying the synchrony measure between each of the communities. The diagonal represents the internal synchrony of each community of oscillators. \label{tab:mat}}
\end{table}

As highlighted in Table \ref{tab:mat}, we focused on the diagonal in each output matrix, which represents the internal synchronisation of each oscillator community. This implies that we are considering that if a community is internally synchronised, then it is also externally synchronised. Once extracted, this diagonal was then discretised by applying a synchronisation threshold representing whether or not the community is considered to be synchronised or not. Though in \cite{Shanahan2010} the only synchronisation threshold used is $0.8$, when analysing our results, we applied thresholds $0.5$, $0.6$, $0.7$, $0.8$, and $0.9$ to each simulation in order to obtain a better view of the effect of varying synchronisation thresholds. As a result, for each time step we obtained an array of booleans, where $1$ represents that a community of oscillators is synchronised and $0$ that it is not. As an example, Table \ref{tab:arr} depicts the application of this procedure to the data shown in Table \ref{tab:mat} using a threshold of $0.6$.

\begin{table}[H]
\centering
\begin{tabular}{l | c c c c c c c c}
& \multicolumn{8}{c}{Community \#} \\ [2mm]
& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
Continuous & 0.29 & 0.56 & 0.20 & 0.77 & 0.35 & 0.64 & 0.84 & 0.68 \\
Discrete & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\
\end{tabular}
\caption{For each time step in a simulation, the extracted diagonal array containing continuous data was converted to discrete data by applying a threshold, in this case $0.6$. \label{tab:arr}}
\end{table}
 
Each of the extracted arrays was then transposed and inserted into an $8 \times 5000$ matrix, with each row representing a community of oscillators and each column a time step. A sample output matrix is show in Table \ref{tab:timeseries}. This provides the final output of each trial as a time series that can be inputted into our implementation of $\Phi$.

\begin{table}[H]
\centering
\begin{tabular}{c l | C{8mm} C{8mm} C{8mm} C{8mm} C{8mm} C{8mm} C{8mm} C{8mm} C{8mm}}

& & \multicolumn{9}{c}{Time Step} \\ [2mm]
& & 1 & 2 & 3 & 4 & . . . & 4997 & 4998 & 4999 & 5000 \\
\hline
\parbox[t]{2mm}{\multirow{8}{*}{\rotatebox[origin=c]{90}{Community \#}}}
& 1 & 0 & 0 & 0 & 0 & . . . & 0 & 0 & 0 & 0 \\
& 2 & 0 & 0 & 0 & 0 & . . . & 1 & 0 & 0 & 1 \\
& 3 & 0 & 0 & 0 & 1 & . . . & 1 & 0 & 0 & 1 \\
& 4 & 0 & 0 & 1 & 1 & . . . & 0 & 0 & 0 & 0 \\
& 5 & 0 & 0 & 0 & 0 & . . . & 0 & 1 & 1 & 1 \\
& 6 & 0 & 1 & 1 & 1 & . . . & 0 & 0 & 1 & 1 \\
& 7 & 0 & 1 & 1 & 0 & . . . & 1 & 1 & 1 & 1 \\
& 8 & 0 & 0 & 0 & 0 & . . . & 1 & 1 & 1 & 1 \\
\end{tabular}
\caption{The final output from each trial was an $8 \times 5000$ matrix representing the time series of which communities were synchronised at each time step in the run. \label{tab:timeseries}}
\end{table}

% ------------
% The Measures
% ------------
\subsubsection{The Measures}
\label{sec:app:osc:measures}

Each simulation we ran was given a unique ID and stored as a document in our database. For each, we then took its output time series and calculated a number of measures, which were recorded in its database entry. Our strategy was to cover the measures used by Shanahan in \cite{Shanahan2010} as well as those associated with our implementation of $\Phi$ and study the relationship between these.\\

\noindent \textbf{Mutual Information ($I$)}\\
\noindent As a sanity check, for each simulation we also computed the mutual information $I(X_{t-\tau}, X_{t})$ for the system $X$ as a whole at state $t$ vs state $t-\tau$. This allowed us to compare how effective information and integrated information behaved vis-\`{a}-vis mutual information, the more traditional information-theoretic measure that they are based on. We calculated $I(X_{t-\tau}, X_{t})$ by using the implementation of Equation \ref{eq:mi} available as part of the JIDT and made available in our effective information calculator as described in Section \ref{sec:impl:ei}.\\

\noindent \textbf{Global Synchrony ($\Psi$)}\\
\noindent The synchrony of a given community $c$ at time $t$, $\psi_c(t)$, is given by the aforementioned Equation \ref{eq:sync}. As explained in Section \ref{sec:bg:global-sync}, this measure can then be averaged over all communities and time steps in order to calculate global synchrony ($\Psi$) \cite{Shanahan2010}. We calculated $\Psi$ for each trial by extracting the continuous synchrony values from each matrix as presented in Table \ref{tab:arr} and inputting them to implementations of Equations \ref{eq:comm-sync} and \ref{eq:global-sync} inside the Python data importer described in Section \ref{sec:utils}.\\

\noindent \textbf{Chimera Index ($\chi$)}\\
\noindent As explained in Section \ref{sec:bg:chimera}, chimera states describe the situation where a group of communities are synchronised while the rest are desynchronised. A measure for this is proposed by Shanahan in \cite{Shanahan2010}. We calculated $\chi$ by extracting the continuous synchrony values from each matrix as presented in Table \ref{tab:arr} and inputting them to implementations of Equations \ref{eq:var-sync} and \ref{eq:chi} inside the Python data importer described in Section \ref{sec:utils}.\\

\noindent \textbf{Metastability Index ($\lambda$)}\\
\noindent We applied the metastability index proposed by Shanahan as $\lambda$ in \cite{Shanahan2010}. This measure computes the average variance in synchrony ($\psi_c(t)$) over time across each community $c$ in the system. We calculated $\lambda$ by extracting the continuous synchrony values from each matrix as presented in Table \ref{tab:arr} and inputting them to implementations of Equations \ref{eq:sigma-met} and \ref{eq:lambda} inside the Python data importer described in Section \ref{sec:utils}.\\

\noindent \textbf{Coalition Entropy ($H_C$)}\\
\noindent Following \cite{Shanahan2010} and \cite{Bhowmik2013}, we measured the coalition entropy ($H_C$) of the systems in our simulations. As presented in Section \ref{sec:bg:hc}, $H_C$ is computed from the possible number of different states that can be reached by the system and the probability of any one of them arising \cite{Bhowmik2013}. We calculated $H_{C}$ by implementing Equation \ref{eq:hc} in our extension for the JIDT as described in Section \ref{sec:impl:hc}.\\

\noindent \textbf{Empirical Integrated Information ($\Phi_{E}$)}\\
\noindent At the core of our analysis, we calculated the empirical integrated information ($\Phi_E$) for each simulation. As explained in Section \ref{II}, empirical integrated information was proposed by Barrett and Seth in \cite{Barrett2011} to compute integrated information for time series data. We calculated $\Phi_{E}$ by implementing Equation \ref{eq:ii} in our extension for the JIDT as described in Section \ref{sec:impl:phi-e}. We also recorded the minimum information bipartition ($MIB$) alongside the value for $\Phi_{E}$. For all calculations of $\Phi_E$ we used a value of $\tau = 1$, which means that we were considering the previous time step, corresponding to a 5ms time difference as described in Section \ref{sec:osc:sims}.\\

\noindent \textbf{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$)}\\
\noindent A second measure we used to calculate integrated information in our study was empirical integrated information tilde ($\widetilde{\Phi}_{E}$), which uses stochastic interaction instead of effective information in order to determine the $MIB$, as detailed in Section \ref{II}. We calculated $\widetilde{\Phi}_{E}$ by implementing Equations \ref{eq:ii-tilde},  \ref{eq:mib-tilde} and \ref{eq:norm-tilde} in our extension for the JIDT as described in Section \ref{sec:impl:phi-e-tilde}. We also recorded its minimum information bipartition alongside the value for $\widetilde{\Phi}_{E}$. As with $\Phi_E$, for all calculations of $\widetilde{\Phi}_{E}$, we used a value of $\tau = 1$.\\

% =======
% RESULTS
% =======
\subsection{Results}
\label{sec:app:osc:res}

After running the 7500 simulations and computing the measures detailed in Section \ref{sec:app:osc:measures}, we analysed various relationships between these measures. First, in order to confirm the consistency of our oscillator simulations with those conducted by Shanahan, we replicated a number of the correlations presented in \cite{Shanahan2010}. This initial step allowed us to be more confident about our results once new parameters and measures were considered. We then extended this analysis by examining the same measures, but covering a wider range of parameters such as the $\beta$ range and synchronisation threshold ($\gamma$). Finally, we built on this analysis to then study the correspondence between the original measures considered in \cite{Shanahan2010} and our implementations of integrated information.

% --------------
% Lambda vs Beta
% --------------
\subsubsection{Metastability Index ($\lambda$) vs $\beta$}
\label{sec:app:osc:res:meta-v-beta}

The first correlation we tested was the one between Shanahan's $\lambda$ measure for metastability, described in Section \ref{sec:bg:lambda}, and $\beta$, the parameter determining the phase lag, as defined in Section \ref{sec:osc:mod}. We plotted the correlation for $\beta$ ranging from $0$ to $\frac{\pi}{4}$ and obtained a result consistent with \cite{Shanahan2010}. As shown in Figure \ref{fig:lambda-vs-beta-orig}, $\lambda$ peaks at approximately $\beta = 0.08$, which falls in the range of $0.05 < \beta < 0.15$ found by Shanahan.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/lambda_vs_beta_orig}
\end{center}
\caption{
	Metastability Index ($\lambda$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
	\label{fig:lambda-vs-beta-orig}
}
\end{figure}

In order to gain a better picture of how $\beta$ and $\lambda$ interacted, we extended our $\beta$ range to cover the complete $0$ to $2\pi$ period. What we encountered was a second peak with an almost symmetrical shape as the peak in the original plot, centred around $\beta = 3.1$. The rest of the range remained flat, with $\lambda$ values close to zero, as depicted in Figure \ref{fig:lambda-vs-beta-ext}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/lambda_vs_beta_ext}
\end{center}
\caption{
	Metastability Index ($\lambda$) vs $\beta$ for $0 \leq \beta \leq 2\pi$.
	\label{fig:lambda-vs-beta-ext}
}
\end{figure}

% -----------
% Chi vs Beta
% -----------
\subsubsection{Chimera Index ($\chi$) vs $\beta$}
\label{sec:app:osc:res:chi-v-beta}

The second correlation we examined corresponded again to a measure presented by Shanahan in \cite{Shanahan2010}. We plotted Shanahan's chimera index ($\chi$), as presented in Section \ref{sec:bg:chi}, versus $\beta$. Once again we started with $\beta$ ranging from $0$ to $\frac{\pi}{4}$, obtaining a result consistent with \cite{Shanahan2010}. As shown in Figure \ref{fig:chi-vs-beta-orig}, $\chi$ peaks at approximately $\beta = 0.08$, which falls in the range of $0.05 < \beta < 0.15$ found by Shanahan.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/chi_vs_beta_orig}
\end{center}
\caption{
	Chimera Index ($\chi$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
	\label{fig:chi-vs-beta-orig}
}
\end{figure}

As with the metastability index, we extended our $\beta$ range to cover $0 \leq \beta \leq 2\pi$. Again we encountered a second peak with an almost symmetrical shape as the one in the original plot. The new apex was also centred around $\beta = 3.1$, with the rest of the range remaining at $\chi$ values close to zero. This is depicted in Figure \ref{fig:chi-vs-beta-ext}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/chi_vs_beta_ext}
\caption{
	Chimera Index ($\chi$) vs $\beta$ for $0 \leq \beta \leq 2\pi$.
	\label{fig:chi-vs-beta-ext}
}
\end{center}
\end{figure}

\subsubsection{Global Synchrony vs $\beta$}
\label{sec:app:osc:res:sync-v-beta}

As synchrony is a key measure in determining the coalitions that are formed during each simulation, it was particularly important that our values for synchrony ($\psi$) and global synchrony ($\Psi$), as defined in Sections \ref{sec:bg:sync} and \ref{sec:bg:global-sync}, were behaving as expected. We thus examined the correlation between $\Psi$ and $\beta$. As shown in Figure \ref{fig:psi-vs-beta-orig}, our results were consistent with \cite{Shanahan2010}, with $\Psi$ tending towards full synchronisation across all communities for $\beta > \frac{\pi}{8}$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/psi_vs_beta_orig}
\caption{
	Global Synchrony ($\Psi$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
	\label{fig:psi-vs-beta-orig}
}
\end{center}
\end{figure}

To continue our analysis, we broadened our $\beta$ range to $0 < \beta < 2\pi$. Once more, we find the symmetry along $\frac{\pi}{2}$ observed for $\lambda$ and $\chi$. As shown in Figure \ref{fig:psi-vs-beta-ext}, after $\beta = \frac{3\pi}{4}$, $\Psi$ starts declining and falls sharply at just under $\beta = \pi$. It then tends towards zero until $\frac{7\pi}{4}$, where it starts rising once more.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/psi_vs_beta_ext}
\caption{
	Global Synchrony ($\Psi$) vs $\beta$ for $0 \leq \beta \leq 2\pi$.
	\label{fig:psi-vs-beta-ext}
}
\end{center}
\end{figure}

% -------------------------
% Coalition Entropy vs Beta
% -------------------------
\subsubsection{Coalition Entropy ($H_C$) vs $\beta$}
\label{sec:app:osc:res:hc-v-beta}

A final correlation considered in \cite{Shanahan2010} that we examined in our project was the one between coalition entropy ($H_C$) and $\beta$. As shown in Figure \ref{fig:hc-vs-beta-orig}, $H_C$ peaks at a $\beta$ value of approximately $0.15$, which is consistent with \cite{Shanahan2010}. Figure \ref{fig:hc-vs-beta-orig} also presents the same broader spread of $H_C$ values around $0.35 < \beta < 0.5$ that can be seen in \cite{Shanahan2010}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/hc_vs_beta_orig}
\caption{
	Coalition Entropy ($H_C$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
	\label{fig:hc-vs-beta-orig}
}
\end{center}
\end{figure}

When we examined the broader range of $0 \leq \beta \leq 2\pi$, we again saw a second symmetrical peak centred on $\beta = 3$, with $H_C$ tending towards zero for $\pi < \beta< 2\pi$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/hc_vs_beta_ext}
\caption{
	Coalition Entropy ($H_C$) vs $\beta$ for $0 \leq \beta \leq 2\pi$.
	\label{fig:hc-vs-beta-ext}
}
\end{center}
\end{figure}

We then extended our analysis to cover multiple thresholds ($\gamma$) for determining whether an oscillator was synchronised or not. As outlined in Section \ref{sec:osc:sims}, we applied thresholds $0.5$, $0.6$, $0.7$, $0.8$, and $0.9$ to each simulation.

As shown in Figure \ref{fig:hc-vs-beta-orig-multi} we can see that as $\gamma$ increases, the peak for $H_C$ shifts to the right and its maximum value decreases slightly. These values are highlighted in Table \ref{tab:max-hc-beta}. Additionally, we observed that the right skew of the peak clearly visible for $\gamma = 0.5$ diminishes as $\gamma$ increases and for $\gamma = 0.9$, the peak appears to be mostly symmetrical.

\begin{table}[H]
\centering
\begin{tabular}{ c | c c }
\multicolumn{3}{c}{Maximum $H_C$} \\ [2mm]
$\gamma$ & $\beta$ & $H_C$\\
\hline
0.5 & 0.056 & 0.979 \\
0.6 & 0.075 & 0.974 \\
0.7 & 0.111 & 0.958 \\
0.8 & 0.147 & 0.954 \\
0.9 & 0.226 & 0.947 \\
\end{tabular}
\caption{
	The values of $\beta$ where coalition entropy ($H_C$) is maximised for each threshold ($\gamma$).
	\label{tab:max-hc-beta}
}
\end{table}

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/hc_vs_beta_orig_multi}
\caption{
	Coalition Entropy ($H_C$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$ at multiple thresholds ($\gamma$).
	\label{fig:hc-vs-beta-orig-multi}
}
\end{center}
\end{figure}

When we extended our $\beta$ range to $0 \leq \beta \leq 2\pi$, we observed the same symmetrical peak for each threshold as previously observed for $\gamma = 0.8$. Similarly, for all thresholds, $H_C$ tends towards zero for $\pi < \beta< 2\pi$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/hc_vs_beta_ext_multi}
\caption{
	Coalition Entropy ($H_C$) vs $\beta$ for $0 \leq \beta \leq 2\pi$ at multiple thresholds ($\gamma$).
	\label{fig:hc-vs-beta-ext-multi}
}
\end{center}
\end{figure}

% -----------
% Phi vs Beta
% -----------
\subsubsection{Empirical Integrated Information ($\Phi_{E}$) vs $\beta$}
\label{sec:app:osc:res:phi-v-beta}

Having completed an analysis of the relationships considered by Shanahan in \cite{Shanahan2010}, we started analysing comparable correlations using our measures of integrated information. To begin with, we considered empirical integrated information ($\Phi_{E}$), as defined in Section \ref{II}, and plotted it against $\beta$.

Figure \ref{fig:phi-vs-beta-orig} shows the correlation between $\Phi_{E}$ and $\beta$ for a range of $0 \leq \beta \leq \frac{\pi}{4}$. We observe that the peak reaches its apex at $\beta \approx 0.16$. This closely follows what we found for $H_C$ when plotted against $\beta$, where $H_C$ reached a maximum at $\beta \approx 0.15$. It is worth noting, however, that the peak for $\Phi_{E}$ versus $\beta$ is narrower than that of $H_C$ versus $\beta$, and that the former, before it starts rising at $\beta \approx 0.07$, presents a dip, where $\Phi_{E}$ reaches values of down to approximately $-0.06$. For $H_C$, this is not possible, given that by definition it cannot take negative values. Additionally, we can see that from $\beta \approx 0.27$ onwards, the value for $\Phi_{E}$ stabilises at close to zero. 

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_beta_orig}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
	\label{fig:phi-vs-beta-orig}
}
\end{center}
\end{figure}

As with the previous measures plotted against $\beta$, we extended the range to $0 \leq \beta \leq 2\pi$. Once more we found a symmetrical peak centred around $\beta \approx 3.1$, with $\Phi_E$ stabilising at around zero for values greater than $\beta \approx 3.17$. This depicted in Figure \ref{fig:phi-vs-beta-ext}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_beta_ext}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs $\beta$ for $0 \leq \beta \leq 2\pi$.
	\label{fig:phi-vs-beta-ext}
}
\end{center}
\end{figure}


When considering multiple thresholds ($\gamma$), we observed a behaviour similar to the one for coalition entropy described in Section \ref{sec:app:osc:res:hc-v-beta}. As $\gamma$ increases, the value of $\beta$ corresponding to the maximum value of $\Phi_E$ also increases. However, the maximum value of $\Phi_E$ doesn't necessarily decrease. It decreases from $\gamma = 0.5$ to $\gamma = 0.7$, but then increases from $\gamma = 0.7$ to $\gamma = 0.9$. This shift and change in maximum $\Phi_E$ can be observed in Figure \ref{fig:phi-vs-beta-orig-multi} and is summarised in Table \ref{tab:max-phi-beta}.

\begin{table}[H]
\centering
\begin{tabular}{ c | c c }
\multicolumn{3}{c}{Maximum $\Phi_E$} \\ [2mm]
$\gamma$ & $\beta$ & $\Phi_E$\\
\hline
0.5 & 0.056 & 0.546 \\
0.6 & 0.082 & 0.525 \\
0.7 & 0.114 & 0.465 \\
0.8 & 0.160 & 0.500 \\
0.9 & 0.238 & 0.632 \\
\end{tabular}
\caption{
	Values of $\beta$ where empirical integrated information ($\Phi_E$) is maximised for each threshold ($\gamma$).
	\label{tab:max-phi-beta}
}
\end{table}

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_beta_orig_multi}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$ at multiple thresholds ($\gamma$).
	\label{fig:phi-vs-beta-orig-multi}
}
\end{center}
\end{figure}

With an extended $\beta$ range, all the multiple thresholds exhibit the same behaviour, with a symmetrical peak appearing at $\beta \approx 3.1$. This can be observed in Figure \ref{fig:phi-vs-beta-ext-multi}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_beta_ext_multi}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs $\beta$ for $0 \leq \beta \leq 2\pi$ at multiple thresholds ($\gamma$).
	\label{fig:phi-vs-beta-ext-multi}
}
\end{center}
\end{figure}


% -----------------
% Phi Tilde vs Beta
% -----------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs $\beta$}
\label{sec:app:osc:res:phi-tilde-v-beta}

We observed comparable results when analysing the relationship between the alternative measure for empirical integrated information, $\widetilde{\Phi}_{E}$, as defined in Section \ref{II}, and $\beta$. When plotting only $\gamma = 0.8$ in Figures \ref{fig:phi-tilde-vs-beta-orig} and \ref{fig:phi-tilde-vs-beta-ext}, we can clearly observe the same peak centred around $\beta \approx 0.15$, with its symmetrical peak on $\beta \approx 3.1$. When plotting multiple thresholds in Figures \ref{fig:phi-tilde-vs-beta-orig-multi} and \ref{fig:phi-tilde-vs-beta-ext-multi}, we observe the same shift and change in maximum $\widetilde{\Phi}_{E}$ values observed in section \ref{sec:app:osc:res:phi-v-beta}.

\begin{figure}[H] 
	\label{fig:phi-tilde-vs-beta-all} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_vs_beta_orig}
		\caption{
				$\widetilde{\Phi}_E$ vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$.
			\label{fig:phi-tilde-vs-beta-orig}
		}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_vs_beta_ext}
		\caption{
			$\widetilde{\Phi}_E$ vs $\beta$ for $0 \leq \beta \leq 2\pi$.
			\label{fig:phi-tilde-vs-beta-ext}
		}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_vs_beta_orig_multi}
		\caption{
			$\widetilde{\Phi}_E$ vs $\beta$ for $0 \leq \beta \leq \frac{\pi}{4}$\\ at multiple $\gamma$.
			\label{fig:phi-tilde-vs-beta-orig-multi}
		}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_vs_beta_ext_multi}
		\caption{
			$\widetilde{\Phi}_E$ vs $\beta$ for $0 \leq \beta \leq 2\pi$\\ at multiple $\gamma$.
			\label{fig:phi-tilde-vs-beta-ext-multi}
		}
		\end{center}
		\vspace{4ex}
	\end{minipage}
\end{figure}

% ----------
% Phi vs Psi
% ----------
\subsubsection{Empirical Integrated Information ($\Phi_{E}$) vs Global Synchrony ($\Psi$)}
\label{sec:app:osc:res:phi-v-psi}

We also considered the relationship between our measures for integrated information and global synchrony ($\Psi$) for multiple thresholds ($\gamma$). For all values of $\gamma$ considered, we observed a parabolic peak with a maximum $\Psi$ ranging from $\Psi \approx 0.44$ to $\Psi \approx 0.81$ increasing with $\gamma$. As with $\Phi_{E}$ versus $\beta$, the value of $\Phi_E$ decreases from $\gamma = 0.5$ to $\gamma = 0.7$, but then increases from $\gamma = 0.7$ to $\gamma = 0.9$. This is depicted in Figure \ref{fig:phi-vs-psi-multi}. It is also worth noting that for $\gamma = 0.6$, $\gamma = 0.7$ and $\gamma = 0.8$, the value of $\Phi_E$ drops below zero before increasing. However, this dip is not symmetrical, as it does not reappear when $\Phi_E$ decreases on the right end of the peak. Finally, as $\gamma$ increases, it is possible to see a shift from a right skew towards a left skew, with $\gamma = 0.8$ showing the most symmetrical peak. 

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_psi_multi}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs Global Synchrony ($\Psi$).
	\label{fig:phi-vs-psi-multi}
}
\end{center}
\end{figure}

% ----------------
% Phi Tilde vs Psi
% ----------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs Global Synchrony ($\Psi$)}
\label{sec:app:osc:res:phi-tilde-v-psi}

As can be seen in Figure \ref{fig:phi-tilde-vs-psi-multi}, the relationship between $\widetilde{\Phi}_{E}$ and $\Psi$ is comparable to the one between $\Phi_E$ and $\Psi$ as described in Section \ref{sec:app:osc:res:phi-v-psi}. The key difference is that since $\widetilde{\Phi}_{E}$ cannot take negative values, there is no dip in $\widetilde{\Phi}_{E}$ preceding the occurrence of the peak for any threshold.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_tilde_vs_psi_multi}
\caption{
	Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Global Synchrony ($\Psi$).
	\label{fig:phi-tilde-vs-psi-multi}
}
\end{center}
\end{figure}

% --------------------------
% Phi vs Metastability Index
% --------------------------
\subsubsection{Empirical Integrated Information ($\Phi_{E}$) vs Metastability Index ($\lambda$)}
\label{sec:app:osc:res:phi-v-lambda}

An important relationship that we were looking to examine was the correlation between our measures of integrated information and measures that indicated the level of metastability exhibited by a given system. We first considered $\Phi_{E}$ and Shanahan's metastability index ($\lambda$), as defined in Section \ref{sec:bg:lambda}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_lambda_multi}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs Metastability Index ($\lambda$).
	\label{fig:phi-vs-lambda-multi}
}
\end{center}
\end{figure}

Initially we plotted the results for all of our values of $\gamma$, as shown in Figure \ref{fig:phi-vs-lambda-multi}. We noticed that the for the highest threshold, $\gamma = 0.9$, the shape of the plot was considerably different than for the rest of the thresholds. Instead of a peak between $\lambda \approx 0.037$ and $\lambda \approx 0.046$, with a  heavy left skew, the peak for $\gamma = 0.9$ was relatively symmetrical, centred around $\lambda \approx 0.023$. At the other end of the spectrum, when $\gamma = 0.5$, the plot did not show a complete peak, but rather a exponential-like curve. These plots are shown in Figures \ref{fig:phi_vs_lambda_5} and \ref{fig:phi_vs_lambda_9}.

\begin{figure}[H] 
	\label{fig:phi-vs-lambda-extremes} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_vs_lambda_5}
		\caption{
			$\Phi_E$ vs $\lambda$ at $\gamma = 0.5$.
			\label{fig:phi_vs_lambda_5}
		}
		\end{center}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_vs_lambda_9}
		\caption{
			$\Phi_E$ vs $\lambda$ at $\gamma = 0.9$.
			\label{fig:phi_vs_lambda_9}
		}
		\end{center}
		\vspace{2ex}
	\end{minipage}
\end{figure}

We then considered thresholds 0.6, 0.7 and 0.8 to better examine the relationship between these two measures without accounting for the \textit{extreme} values of $\gamma$. We observe a heavily left skewed peak that shifts left as the threshold increases, reaching a maximum of $\Phi_E \approx 0.5$. It is also important to note that there is a considerable amount of noise in the range of $0.025 < \lambda < 0.045$. In this region, values for $\lambda$ that produce relatively high values of $\Phi$ also lead to negative values.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_lambda_mid}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs Metastability Index ($\lambda$) at mid-range synchronisation thresholds ($\gamma$).
	\label{fig:phi-vs-lambda-mid}
}
\end{center}
\end{figure}

% --------------------------------
% Phi Tilde vs Metastability Index
% --------------------------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs Metastability Index ($\lambda$)}
\label{sec:app:osc:res:phi-tilde-v-lambda}

We proceeded to extend our analysis between integrated information and Shanahan's metastability index to our implementation of $\widetilde{\Phi}_{E}$. We found the same discrepancies at \textit{extreme} thresholds described in Section \ref{sec:app:osc:res:phi-v-lambda} so we focused on plotting the correlation given $\gamma$ values of 0.6, 0.7 and 0.8. We can see in Figure \ref{fig:phi-tilde-vs-lambda-mid} that the correlation is comparable to the one using $\Phi_E$, with slightly less noise occurring in the range $0.025 < \lambda < 0.045$ and specifically for $\gamma = 0.6$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_tilde_vs_lambda_mid}
\caption{
	Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Metastability Index ($\lambda$) at mid-range synchronisation thresholds ($\gamma$).
	\label{fig:phi-tilde-vs-lambda-mid}
}
\end{center}
\end{figure}

% --------------------
% Phi vs Chimera Index
% --------------------
\subsubsection{Empirical Integrated Information ($\Phi_{E}$) vs Chimera Index ($\chi$)}
\label{sec:app:osc:res:phi-v-chi}

A second measure related to the metastability of a system is Shanahan's chimera index as defined in Section \ref{sec:bg:chi}. We considered the relationship between $\chi$ and $\Phi_{E}$ and confirmed a nearly identical behaviour as that between $\Phi_E$ and $\lambda$. These findings are consistent with the close relationship between $\chi$ and $\lambda$ presented in \cite{Shanahan2010} and in our results in Sections \ref{sec:app:osc:res:meta-v-beta} and \ref{sec:app:osc:res:chi-v-beta}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_chi_multi}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs Chimera Index ($\chi$).
	\label{fig:phi-vs-chi-multi}
}
\end{center}
\end{figure}

Once more, we first created a plot for results encompassing all of the values of $\gamma$ used in our simulations, as shown in Figure \ref{fig:phi-vs-chi-multi}. As with $\lambda$ the \textit{extreme} values of $\gamma = 0.5$ and $\gamma = 0.9$ were not particularly in line with the rest of the values of $\gamma$. These plots are shown in Figures \ref{fig:phi_vs_chi_5} and \ref{fig:phi_vs_chi_9}.

\begin{figure}[H] 
	\label{fig:phi-vs-chi-extremes} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_vs_chi_5}
		\caption{
			$\Phi_E$ vs $\lambda$ at $\gamma = 0.5$.
			\label{fig:phi_vs_chi_5}
		}
		\end{center}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_vs_chi_9}
		\caption{
			$\Phi_E$ vs $\lambda$ at $\gamma = 0.9$.
			\label{fig:phi_vs_chi_9}
		}
		\end{center}
		\vspace{2ex}
	\end{minipage}
\end{figure}

We thus plotted thresholds 0.6, 0.7 and 0.8 separately, obtaining the same results as in Section \ref{sec:app:osc:res:phi-v-lambda}. Once more, the plots showed a heavily left skewed peak, shifting towards the left as $\gamma$ increases and reaching a maximum of $\Phi_E \approx 0.5$. The noise in the range of $0.025 < \lambda < 0.045$ was also present, with $\chi$ values producing both relatively high $\Phi$ as well as negative values.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_chi_mid}
\caption{
	Empirical Integrated Information ($\Phi_E$) vs Chimera Index ($\chi$) at mid-range synchronisation thresholds ($\gamma$).
	\label{fig:phi-vs-chi-mid}
}
\end{center}
\end{figure}

% --------------------------
% Phi Tilde vs Chimera Index
% --------------------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs Chimera Index ($\chi$)}
\label{sec:app:osc:res:phi-tilde-v-chi}

As with all our analyses involving integrated information, we then plotted the same relationship, but using our implementation of $\widetilde{\Phi}_{E}$. The same discrepancies at \textit{extreme} thresholds reappeared and so we once more plotted the correlation for $\gamma$ values of 0.6, 0.7 and 0.8 only. Figure \ref{fig:phi-tilde-vs-lambda-mid} shows that the correlation closely resembles the one using $\Phi_E$, with the same reduction of noise within the range $0.025 < \lambda 0.045$ for $\gamma = 0.6$ as observed for $\Phi_E$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_tilde_vs_chi_mid}
\caption{
	Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Chimera Index ($\chi$) at mid-range synchronisation thresholds ($\gamma$).
	\label{fig:phi-tilde-vs-chi-mid}
}
\end{center}
\end{figure}

% ------------------------
% Phi vs Coalition Entropy
% ------------------------
\subsubsection{Empirical Integrated Information ($\Phi_{E}$) vs Coalition Entropy ($H_C$)}
\label{sec:app:osc:res:phi-v-hc}

We continued our analysis by exploring the relationship between integrated information and coalition entropy ($H_C$). As defined in Section \ref{sec:bg:hc}, coalition entropy measures the occurrence of the various states entered by a system given the space of all possible states it can enter. Nevertheless, coalition entropy does not take into account any past state given a current state, and so it is agnostic to the order in which the states actually occur. This is implied in its definition in Equation \ref{eq:hc}. As integrated information does take into consideration the order in which states occur, it is of particular interest to analyse its relationship with $H_C$.

We first plotted $\Phi_E$ against $H_C$, as shown in Figure \ref{fig:phi-vs-hc-multi}. We can see that for all thresholds, the plots form an exponential-like pattern. Obviating $\gamma = 0.9$, we observe that there is a heavy overlap between the various thresholds, with all of them reaching a maximum of $\Phi_E \approx 0.5$ as $H_C$ approaches 1.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_vs_hc_multi}
\caption{
		Empirical Integrated Information ($\Phi_E$) vs Coalition Entropy ($H_C$).
	\label{fig:phi-vs-hc-multi}
}
\end{center}
\end{figure}

% ------------------------------
% Phi Tilde vs Coalition Entropy
% ------------------------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs Coalition Entropy ($H_C$)}
\label{sec:app:osc:res:phi-tilde-v-hc}

We then examined $\widetilde{\Phi}_{E}$ against $H_C$, as shown in Figure \ref{fig:phi-tilde-vs-hc-multi}. The result is similar to the one for $\Phi_E$ in Section \ref{sec:app:osc:res:phi-v-hc}, with data from all thresholds forming an exponential-like pattern. Once more, if we ignore $\gamma = 0.9$, there is a consistent overlap between all other values of $\gamma$. Additionally, aside from $\gamma = 0.9$, there is considerably less noise, with only a few outliers at $H_C \approx 0.6$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_tilde_vs_hc_multi}
\caption{
	Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Coalition Entropy ($H_C$).
	\label{fig:phi-tilde-vs-hc-multi}
}
\end{center}
\end{figure}

% ----------------
% Phi Tilde vs Phi
% ----------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_{E}$) vs Empirical Integrated Information ($\Phi_{E}$)}

We were also interested in investigating the relationship between $\Phi_{E}$ and $\widetilde{\Phi}_{E}$. Thus, for each simulation, we plotted the value we obtained for $\widetilde{\Phi}_{E}$ against the one for $\Phi_E$, as shown in Figure \ref{fig:phi-tilde-vs-phi}. We observed a general positive correlation, with $\widetilde{\Phi}_{E}$ being higher than its respective $\Phi_E$ value, as can be observed by the fact that the plotted points are consistently above the $\widetilde{\Phi}_{E} = \Phi_E$ line in the dashed black line. Additionally, we can see that for values of $\Phi_E \approx 0$, there was more inconsistency with regards to the corresponding value $\widetilde{\Phi}_{E}$.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/phi_tilde_vs_phi}
\caption{
	Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Empirical Integrated Information ($\Phi_E$).
	\label{fig:phi-tilde-vs-phi}
}
\end{center}
\end{figure}

% =======================
% Surrogate Data Analysis
% =======================
\subsection{Surrogate Data Analysis}
\label{sec:app:osc:surrogate}

In order to provide insight into the robustness of our results concerning integrated information, we conducted some analysis using surrogate data. For our purposes we used both transformations of data generated by the oscillator simulations as well as synthetic data generated based on predefined models. We examined not only how our surrogate data analysis affected $\Phi_E$ and $\widetilde{\Phi}_E$, but also mutual information, as this measure is  the key building block of $\Phi_E$.

% ------
% Sorted
% ------
\subsubsection{Sorted Time Series}
\label{sec:app:osc:surrogate:sorted}

In order to sort the time series by state, we took advantage of the \texttt{reduce} method in our \texttt{Input} class. As described in Section \ref{sec:impl:ei}, \texttt{reduce} takes the two-dimensional time series matrix and assigns a unique state integer for each possible vector value. The output is a one-dimensional array of states, which we could then sort using Java's inbuilt \texttt{Arrays.sort} method. We then computed mutual information, coalition entropy, empirical integrated information and empirical integrated information tilde for each of the 7500 simulations and stored the results for comparison with the original values.

As expected, coalition entropy was not affected by the sorting operation, given that it is agnostic to the order in which each coalition appears, as discussed in Section \ref{sec:app:osc:res:phi-v-hc}. This is shown in Figure \ref{fig:hc_sorted_vs_hc}.

\begin{figure}[H]
\begin{center}
\includegraphics[scale = 0.35]{figures/hc_sorted_vs_hc}
\caption{
	Coalition Entropy Sorted ($H'_C$) vs Coalition Entropy ($H_C$).
	\label{fig:hc_sorted_vs_hc}
}
\end{center}
\end{figure}

Regarding empirical integrated information, Figure \ref{fig:phi_sorted_vs_phi} shows how as the original value for $\Phi_E$ increases, the value for the sorted run, $\Phi'_E$ decreases. In fact, the vast majority of value for $\Phi'_E$ are negative. A similar phenomenon can be seen for $\widetilde{\Phi}_E$ as shown in Figure \ref{fig:phi_tilde_sorted_vs_phi_tilde}. In this case, it is possible that the sorting operation introduced nonstationarities to the joint probability distribution function, which caused the usually-positive stochastic information measure to be negative \cite{Barrett2011}. This is not directly relevant to our current analysis, but deserves future investigation.

\begin{figure}[H] 
	\label{fig:phi-vs-phi-sorted} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_sorted_vs_phi}
		\caption{
			$\Phi'_E$ vs $\Phi_E$.
			\label{fig:phi_sorted_vs_phi}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_sorted_vs_phi_tilde}
		\caption{
			$\widetilde{\Phi}'_E$ vs $\widetilde{\Phi}_E$.
			\label{fig:phi_tilde_sorted_vs_phi_tilde}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
\end{figure}


We can also see in Figure \ref{fig:phi_sorted_vs_hc} how $\Phi'_E$ decreases as coalition entropy increases. For $\widetilde{\Phi}'_E$, this relationship is even more demarcated and consistent across all thresholds, as shown in Figure \ref{fig:phi_tilde_sorted_vs_hc}. Once more we observe negative $\widetilde{\Phi}'_E$ values, which we hypothesise are caused by the sorting operation removing certain assumptions required for stochastic interaction to be always positive.

\begin{figure}[H] 
	\label{fig:phi-vs-hc-sorted} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_sorted_vs_hc}
		\caption{
			$\Phi'_E$ vs $H_C$.
			\label{fig:phi_sorted_vs_hc}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_sorted_vs_hc}
		\caption{
			$\widetilde{\Phi}'_E$ vs $H_C$.
			\label{fig:phi_tilde_sorted_vs_hc}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
\end{figure}

% --------
% Shuffled
% --------
\subsubsection{Shuffled Time Series}
\label{sec:app:osc:surrogate:shuffled}

In order to shuffle the time series by state, used the \texttt{shuffle} method that we implemented as a helper method for the JIDT's \texttt{MatrixUtils} class. As described in Section \ref{sec:impl:shuffle}, \texttt{shuffle} takes a two-dimensional matrix and rearranges its vectors using Durstenfeld's version of the Fisher-Yates shuffle \cite{Durstenfeld1964}. We then computed the mutual information, coalition entropy, empirical integrated information and empirical integrated information tilde for each of the 7500 simulations in their shuffled state and stored the results for comparison with the original values.

Like for the sorted analysis described in Section \ref{sec:app:osc:surrogate:sorted}, coalition entropy was not affected by the shuffling operation, given that it does not take into account the order in which states appear, but their frequency. Hence the plot for the shuffled coalition entropy versus the original values is identical to the one shown in Figure \ref{fig:hc_sorted_vs_hc}.

Figure \ref{fig:phi_shuffled_vs_phi} shows how their is a positive correlation between the original value for $\Phi_E$ and the one for the shuffled run ($\Phi''_E$). In fact, the value of $\Phi''_E$ is consistently more than double the one for the original $\Phi_E$. This implies that shuffling the states of the times series randomly actually increases the integrated information of the system. A similar phenomenon can be seen for $\widetilde{\Phi}_E$ as shown in Figure \ref{fig:phi_tilde_shuffled_vs_phi_tilde}. 

\begin{figure}[H] 
	\label{fig:phi-vs-phi-shuffled} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_shuffled_vs_phi}
		\caption{
			$\Phi''_E$ vs $\Phi_E$.
			\label{fig:phi_shuffled_vs_phi}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_shuffled_vs_phi_tilde}
		\caption{
			$\widetilde{\Phi}''_E$ vs $\widetilde{\Phi}_E$.
			\label{fig:phi_tilde_shuffled_vs_phi_tilde}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
\end{figure}

Additionally, Figures \ref{fig:phi_shuffled_vs_hc} and \ref{fig:phi_tilde_shuffled_vs_hc} show how both $\Phi''_E$ and $\widetilde{\Phi}''_E$, respectively, follow a similar exponential-like increase versus coalition entropy to the one that we observed for the original data.

\begin{figure}[H] 
	\label{fig:phi-vs-hc-shuffled} 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_shuffled_vs_hc}
		\caption{
			$\Phi''_E$ vs $H_C$.
			\label{fig:phi_shuffled_vs_hc}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/phi_tilde_shuffled_vs_hc}
		\caption{
			$\widetilde{\Phi}''_E$ vs $H_C$.
			\label{fig:phi_tilde_shuffled_vs_hc}
		}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
\end{figure}

% ---------
% Synthetic
% ---------
\subsubsection{Synthetic Time Series}
\label{sec:app:osc:surrogate:synthetic}

In addition to using rearranged versions of the data outputted by the simulations, we also generated synthetic data using predefined patterns and models to test the behaviour of our implementation. The data was generated through one of our Python utilities described in Section \ref{sec:utils} and inserted into our MongoDB data store in order to allow flexible manipulation for repeated analysis. We ran a total of 108 synthetic data simulations each lasting 5000 time steps and incorporating three variables representing communities of oscillators. This means that each system could reach a maximum of $2^3$ states. For all simulations we used a $\tau$ value of one.

Our first set of synthetic data were repeating patterns of one to eight states. We first calculated the empirical integrated information, mutual information and coalition entropy for the states oscillating in order and then repeated our calculations for a shuffled version of the system. Figure \ref{fig:synth_difference_shuffled} shows the difference (Value for Original Data minus Value for Shuffled Data) between the three measures we calculated for according to the length of the pattern.

\begin{figure}[H]
	\begin{center}
		\includegraphics[scale = 0.35]{figures/synth_difference_shuffled}
		\caption{
			Surrogate Analysis with Repeating Patterns.
			\label{fig:synth_difference_shuffled}
		}
	\end{center}
\end{figure}

As expected, coalition entropy does not change, as it is agnostic to the order in which states appear. The difference in mutual information, on the other hand, increases steadily as the length of the pattern increases. We observe that the longer the pattern, the larger the difference between the original mutual information and the shuffled mutual information, with the former being the larger value. For empirical integrated information, the results are not as consistent. For patterns of length two and three, the shuffled empirical integrated information was actually larger than that for the original data. For patterns of lengths four to eight, the opposite was true, though there is no clear trend as to whether the difference increased or decreased with length.

A second set of synthetic data were randomly generated time series. Once more we first calculated the empirical integrated information, mutual information and coalition entropy for the original random time series and then repeated the calculation once the system had been shuffled. As we can see in Figure \ref{fig:synth_random}, there is no correlation between the original and shuffled data both in terms of mutual information and empirical integrated information. Furthermore, the values for both measures are very low, ranging from approximately $0.004$ to $0.011$ for mutual information and $0.002$ to $0.008$ for empirical integrated information. 

\begin{figure}[H] 	
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/synth_random_mi}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{figure}[H]
		\begin{center}
		\includegraphics[scale = 0.2]{figures/synth_random_phi}
		\end{center}
		\end{figure}
		\vspace{2ex}
	\end{minipage}
	\caption{
		 (Left) Mutual Information and (Right) Empirical Integrated Information ($\Phi_E$) Analysis for Random Surrogate Data.
		 \label{fig:synth_random} 
	}
\end{figure}

A final synthetic data simulation was based on a generative model adapted from \cite{WikimediaFoundation2015}. We considered a network with two binary members. This means that the system can be in one of $2^2 = 4$ possible states. We initialise the system to state [0, 0] and allow the first member to take on a random state at any given time step $t$. At time $t$, the second member will take on whatever the value the first element had on time $t-1$. As explained in \cite{WikimediaFoundation2015}, the effective information generated by a system modelled in this fashion is equal to one bit.

To be consistent with our other simulations, we ran the generative model for 5000 time steps and then computed the effective information ($\phi$) for each partition. As expected, the resulting $\phi$ for both partitions was equal and tending towards one. To confirm this we ran the model for 1000000 time steps and indeed this convergence is clear. Since both partitions' $\phi$ is equal, empirical integrated information $\Phi$ will be equal as well.

We then shuffled the time series and repeated our calculations. The effective information then fell close to zero, converging quickly as confirmed by the second run with a duration of 1000000 time steps. Again since both partitions' $\phi$ is equal, empirical integrated information $\Phi$ will be equal as well at $\Phi_E \approx 0$. Our results are summarised in Table \ref{tab:synth-generative}.

\begin{table}[H]
\centering
\begin{tabular}{ c | c c }
\multicolumn{3}{c}{Value of $\phi$ and $\Phi_E$} \\ [2mm]
Duration (time steps) & 5000 & 1000000\\
\hline
Original & 0.9998969901985634 & 1.0000001357487622 \\
Shuffled & 0.0011835995533143967 & 4.486560050455113E-6\\
\end{tabular}
\caption{
	The values of $\phi$ and $\Phi_E$ for a generative model, both before and after shuffling.
	\label{tab:synth-generative}
}
\end{table}

\subsection{Discussion}

As can be evidenced by the correlations explained in Section \ref{sec:app:osc:res} and the surrogate data analysis in Section \ref{sec:app:osc:surrogate}, there are a number of conclusions that we can derive from our results. Similarly, there are certain areas where further analysis could provide more insight into the behaviour of empirical integrated information in populations of Kuramoto oscillator communities.

To begin with, it is important to note that we were able to closely replicate all of the correlations presented in \cite{Shanahan2010} as depicted in Figures \ref{fig:lambda-vs-beta-orig}, \ref{fig:chi-vs-beta-orig}, \ref{fig:psi-vs-beta-orig} and \ref{fig:hc-vs-beta-orig}. This allows us to verify that our simulations behaved analogously to those used in the original study, which provides a stronger base for our conclusions about the behaviour of $\Phi_E$ and $\widetilde{\Phi}_E$. 

When we then extended our analysis of the original measures to cover a wider range of $\beta$ and thresholds $\gamma$, we discovered certain unexpected behaviour that is worth investigating in further studies. When extending the $\beta$ range from the original $0 \leq \beta \leq \frac{\pi}{4}$ to $0 \leq \beta \leq 2\pi$, we noticed that the measures for metastability index, chimera index, global synchrony and coalition entropy all exhibited a symmetric shape between $0 \leq \beta \leq \pi$ about $\frac{\pi}{2}$, followed by a consistent value close to $0$ from $\pi$ to $2\pi$. This can be seen in Figures \ref{fig:lambda-vs-beta-ext}, \ref{fig:chi-vs-beta-ext}, \ref{fig:psi-vs-beta-ext} and \ref{fig:hc-vs-beta-ext}. Although not directly related to the present study, the occurrence of this phenomenon deserves further attention in possible future work. In parallel, we also extended the number of synchronisation thresholds  $\gamma$ and investigated their impact on the correlation between coalition entropy and $\beta$ presented in \cite{Shanahan2010}. As shown in Figure \ref{fig:hc-vs-beta-orig-multi}, as $\gamma$ increased, the peak for the curve shifted right and slightly downward, becoming less right skewed. Once more, this is not directly related to our analysis, but deserves further attention to fully understand the relationship between $H_C$ and $\beta$ at different levels of $\gamma$.

The key part of our analysis was inspecting the correlations between our implementations of empirical integrated information and the other measures included in \cite{Shanahan2010}. We found certain consistencies that are worth noting. First of all, we noticed that $\Phi_E$, at $\gamma = 0.8$ when plotted agains $\beta$ peaks at $\beta \approx 0.16$, which coincides with $\beta \approx 0.15$, where $H_C$ peaks. This relationship is consistent, with minor shifts, across all tested values of $\gamma$. A close look at Figure \ref{fig:phi-vs-hc-multi} shows how there is a positive correlation between $H_C$ and $\Phi_E$ with a close to an exponential shape. We can thus infer that higher levels of coalition entropy, where the system visits more states with more equal probability, provide a better environment for higher values of integrated information.

We also found a correlation between $\Phi_E$ and global synchrony ($\Psi$), forming mostly symmetrical peaks that shift to the right as $\gamma$ increases. Nevertheless, all peaks tend towards 0 as $\Psi$ approaches a full synchronisation value of 1. We can infer that a system tending towards full synchronisation does not leave much room for any of its subsystems to act differently from the whole, reducing the potential of the system to integrate information. This is consistent with the definition of $\Phi$ as the ability of a system to generate information as a whole rather than by the sum of its parts, which we discussed in Section \ref{sec:intro}.

Most importantly, we observed a relationship between $\Phi_E$ and both the metastability index ($\lambda$) and the chimera index ($\chi$). In both cases, the relationship is more demarcated for mid-range $\gamma$ values (0.6 through 0.8), with heavily left skewed peaks reaching a maximum value of $\Phi_E$ for values of $\lambda$ and $\chi$ around 0.38. Comparatively, both $\lambda$ and $\chi$, at $\gamma = 0.8$, have maximum values of approximately 0.45. This maximum value $\Phi_E$ for $\gamma = 0.8$, which occurs at $\lambda$ and $\chi$ values of approximately 0.38, then tends towards zero as $\lambda$ and $\chi$ approach 0.45. This could indicate that there is an increase in $\Phi$ as metastability and chimera states occur in a system, but that the $\Phi$ is not maximised when these are present at their fullest. Additionally, this could underline the importance of the link between $\Phi_E$ and $\beta$, as for $\gamma = 0.8$, $\Phi_E$ peaks at $\beta \approx 0.16$, whereas $\lambda$ and $\chi$ both peak at $\beta \approx 0.8$. This slight offset may provide a window into the relationship between metastability and integrated information and deserves further attention in future studies.

Another noteworthy finding is the consistency between $\Phi_E$ and its variation using stochastic interaction $\widetilde{\Phi}_E$. As shown in Figure \ref{fig:phi-tilde-vs-phi}, although $\widetilde{\Phi}_{E}$ gives consistently higher values than $\Phi_E$, the shape of the plot is closely parallel to the line $\Phi_E = \widetilde{\Phi}_{E}$. Additionally, all the relationships observed between $\Phi_E$ and other measures were also found using $\widetilde{\Phi}_{E}$, with the exception, by definition, that $\widetilde{\Phi}_{E}$ did not take negative values. Nonetheless, it is crucial that we further investigate the relationship between both variations at low values of $\Phi_E$. Our findings show that for values of $\Phi_E \approx 0$, there was more inconsistency with regards to the corresponding value $\widetilde{\Phi}_{E}$. This is an observation worth taking into consideration when using both values interchangeably.

Finally, we need to draw attention to our surrogate data analysis. Although certain results were as expected, we also observed a number of particularly bizarre behaviours. Most importantly, we saw in Section \ref{sec:app:osc:surrogate:shuffled} that as we shuffled our time series data, we obtained higher values of $\Phi_E$ than for the original input. As discussed in Section \ref{sec:app:osc:surrogate:synthetic}, an analysis using synthetic data showed that for very short repeating patterns, the original $\Phi_E$ was lower than the shuffled version. This could be related to the unusual results concerning the shuffled time series data. We propose a more in-depth surrogate data analysis in Section \ref{sec:fw:surrogate} to shed light on this matter.

\clearpage

% =======================================================================================
% APPLICATION TO POPULATIONS OF SPIKING NEURONS
% =======================================================================================
\section{Application to Populations of Spiking Neurons}
\label{MSUSNN}
By modelling synchronisation using actual spiking neurons, we gain access to more fine-grained properties of the network. Additionally, achieving metastable and chimera states using spiking neural networks is more biologically relevant than using the large-scale approximations through communities of Kuramoto oscillators. It was therefore evident that we should further our project by applying our implementation of empirical integrated information ($\Phi_E$) to networks of spiking neurons.

\subsection{Methodology}
\label{sec:snn:methods}

Based on work by Bhowmik and Shanahan \cite{Bhowmik2013}, we modelled populations of neurons that exhibit synchrony, metastability and chimera states. Our aim was to run simulations on networks constructed following the model proposed in \cite{Bhowmik2013} to generate discrete time series output that could be then inputted into our implementations of $\Phi_E$ as described in Section \ref{sec:impl}. As with the analysis in Section \ref{MSUKO}, we were keen to draw meaningful connections between the data analysis on our simulations and the conclusions presented in \cite{Bhowmik2013}. We thus replicated, where possible, the methodology, methods and calculations used by Bhowmik and Shanahan, as explained in this section.

% ---------
% The Model
% ---------
\subsubsection{The Model}
\label{sec:snn:mod}

The model used by Bhowmik and Shanahan is depicted in Figure \ref{Bhowmik2013_Schema}. As a basis, their architecture uses Quadratic Integrate-and-Fire (QIF) neurons displaying Hodgkin-Huxley Type I neuron dynamics, as previously explained in Section \ref{sec:bg:snn}.  In order to achieve oscillatory patterns, they arranged the neural populations in a pyramidal inter-neuronal gamma (PING) architecture. Each PING oscillator consisted of 200 neurons making up its excitatory layer as well as 100 neurons making up its inhibitory layer, with only the excitatory layer connected to other communities. Connections were made between two inhibitory neurons (II) and between inhibitory and excitatory neurons (IE and EI), but not between two excitatory neurons in order to avoid saturation effects \cite{Bhowmik 2013}. These connections are shown in Figure \ref{Bhowmik2013_Schema}.

\begin{figure}[H]
\centering
\includegraphics[scale = 0.75]{Bhowmik2013_Schema}
\caption{``The pyramidal inter-neuronal gamma (PING) architecture used for the neural oscillator nodes in the simulation experiments. To generate oscillator nodes of different frequencies for different neural models this base architecture was used with a genetic algorithm evolving the weights and delays for the synaptic connections.'' \cite{Bhowmik2013}}
\label{Bhowmik2013_Schema}
\end{figure}

For the simulations in \cite{Bhowmik2013}, ten PING oscillators were used, each with frequencies between 30Hz and 50Hz. Two parameters, $P$ and $W$, which took a random value between 0 and 1, determined the number and strength of the connections made between the different oscillators. The parameter $P$ determined the probability of a one-way connection between one oscillator and another, that is, between their excitatory layers. If a connection was present, then 20\% of the neurons in one oscillator would provide input to 20\% of the neurons in the recipient oscillator. The parameter $W$ determined the weight of the synaptic connection, which was fixed for all connections. External stimulus was provided by a Poisson process \cite{Bhowmik2013}.

% ---------------
% The Simulations
% ---------------
\subsubsection{The Simulations}
\label{sec:snn:sims}
In order to generate the time series that were needed to apply our implementation of $\Phi$ to the populations of spiking neural networks, we performed our own simulations using the Matlab code from \cite{Bhowmik2013} as a base. We kept the default parameters and structure explained in Section \ref{sec:snn:mod}, running 1003 simulations each lasting 5000 time steps.

Each simulation outputted a Matlab file containing information about the coalition entropy ($H_C$), global synchrony ($\Psi$), metastability index ($\lambda$), synaptic connection weight ($W$) and probability ($P$) of the system, amongst other measures. Alongside these measures, the output contained an array containing the coalitions formed at each time step, which were designated as subarrays. A sample format of this output is shown in Table \ref{tab:snn:output}.

\begin{table}[H]
\centering

\begin{tabular}{c r | l l l l }

& & \multicolumn{4}{c}{\textbf{Coalitions}} \\ [2mm]
& & \multicolumn{4}{c}{(by \# of PING Oscillator)} \\ [2mm]
\hline
\parbox[t]{2mm}{\multirow{9}{*}{\rotatebox[origin=c]{90}{\textbf{Time Step}}}}
& 1 & 		[[1 2 3 6 7 8 9 10], & [4 5]] & & \\
& 2 & 		[[2 3 6 7 8 9 10], & [1 4 5]] & & \\
& 3 & 		[[2 7 10], & [3 6 9], & [8], & [1 4 5]] \\
& 4 & 		[[2 6 7 10], & [3 9], & [1 4 8], & [5]] \\
& ... & 	& . . . & &  \\
& 4997 & 	[[3 7 9 10], & [1 8], & [2 4 5 6]] &  \\
& 4998 & 	[[3 7 9 10], & [1], & [2 4 5 6], & [8]] \\
& 4999 & 	[[1 3 7 9 10], & [2 4 5 6 8]] & & \\
& 5000 & 	[[1 7 9 10], & [2 4 6 8], & [3 5]] & \\
\end{tabular}
\caption{Each trial outputted an array of $5000$ arrays whose elements indicated which PING oscillators were synchronised and thus formed a coalition. \label{tab:snn:output}}
\end{table}

Since the format presented in Table \ref{tab:snn:output} was not compatible with our empirical integrated information calculators, we needed to implement a utility in Python that would load the Matlab files, transform the data into a usable format and store it in our database. The first step for this transformation was to select, for each time step, a group of PING oscillators to define as synchronised, that is assign a $1$ in our time series, and assign $0$ to the rest. This required some decision-making as to how to treat time steps where more than two, possible equally large coalitions, were present in the output, such as can be seen in time steps 4, 4997, 4998, 4999 and 5000 in Table \ref{tab:snn:output}. Our approach was to follow the algorithm put forth below.

\begin{enumerate}
\item{Select the largest coalition at time step $t$.}
\item{If there are two or more equally large coalitions, select the one that is most similar to the coalition selected in time step $t - 1$. This was decided by computing the size of the intersect between the arrays.}
\item{If one or more coalitions are equally similar to the coalition selected in time step $t-1$, select one at random.}
\item{Repeat for all time steps.}
\end{enumerate}

This algorithm allowed us to transform the data to a time series matrix of a form identical to the one in Table \ref{tab:timeseries}, which was compatible with our integrated information calculators and could be stored in our database.

% ------------
% The Measures
% ------------
\subsubsection{The Measures}
\label{sec:app:snn:measures}

As in our simulations using Kuramoto oscillators, each simulation was given a unique ID and stored as a document in our database. For each, we stored some of the measures readily available from the data files exported by the Matlab simulations based on the code provided in \cite{Bhowmik2013}. Additionally, we computed a number of measures, which were recorded in its database entry. Our strategy was to compute as many of the measures that were included in the application to the Kuramoto oscillator populations in order to provide a comparable analysis. A list of the measures is provided below. The computation carried out for mutual information, empirical integrated information and empirical integrated information tilde is comparable to the process explained in Section \ref{sec:app:osc:measures}. The other measures were available in the Matlab data files on import.

\begin{enumerate}
\item{Mutual Information (Equation \ref{eq:mi})}
\item{Global Synchrony (Equation \ref{eq:sync})}
\item{Metastability Index (Equation \ref{eq:lambda})}.
\item{Coalition Entropy (For consistency in the comparison with the original results, we used the equation for coalition entropy as defined in \cite{Bhowmik2013}, which differs from our implementation in Equation \ref{eq:hc}.)}
\item{Empirical Integrated Information (Equation \ref{eq:ii})}
\item{Empirical Integrated Information Tilde (Equation \ref{eq:ii-tilde})}
\end{enumerate}

% =======
% RESULTS
% =======
\subsection{Results}
\label{sec:snn:res}

Our 1003 simulations were divided into two groups. The first, consisting of 472 simulations was given a synchronisation threshold ($\gamma$) of $0.8$, while for the second, comprising the other 531 simulations, $\gamma$ was equal to $0.95$. The synchronisation threshold, as in our application to Kuramoto oscillators, was used to discretise whether or not two communities or more were in synchrony. That is, only communities having a level of synchronisation equal to or higher than $\gamma$ would be considered to be synchronised. Having run the simulations and computing the measures detailed in Section \ref{sec:app:snn:measures}, we proceeded to analyse the various relationships that were of interest to us. First, we aimed to confirm the consistency of our spiking neural network simulations with those conducted by Bhowmik and Shanahan. We thus replicated a selection of the correlations presented in \cite{Bhowmik2013}. We then extended our analysis by examining the the correlations between the original measures considered and our implementations of integrated information. For our calculations involving empirical integrated information we used a time step offset ($\tau$) value of both $1$ and $5$.

\subsubsection{Global Synchrony ($\Psi$) vs Connection Probability $(P)$ and Weight ($W$)}
\label{sec:app:snn:res:sync}
The first correlation we tested concerned global synchrony ($\Psi$), which we describe in detail in Section \ref{sec:bg:sync}. We plotted $\Psi$ against the synaptic connection probability ($P$) and weight ($W$), the parameters determining the number and strength of the connections between PING oscillators, as defined in Section \ref{sec:snn:mod}. We obtained a result in line with \cite{Bhowmik2013}, with $\Psi$ increasing consistently with $P$. However, we did not observe that $\Psi$ also increase with $W$ except at already high values of $P$. Our increase in $\Psi$ at high levels of $P$ and $W$ was also far lower than the levels observed in \cite{Bhowmik2013}, with ours being $\Psi \approx 0.55$ while Bhowmik and Shanahan's $\Psi \approx 1$. As shown in Figure \ref{fig:p_w_psi_all}, this relationship is present both at thresholds of $\gamma = 0.8$ and $\gamma = 0.95$.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_psi_8_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_psi_95_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
			Global Synchrony ($\Psi$) vs Connection Probability $(P)$ and Weight ($W$)
			\label{fig:p_w_psi_all}
		}
\end{figure}

% Coalition Entropy vs Probability and Weight
% -------------------------------------------
\subsubsection{Coalition Entropy ($H_C$) vs Connection Probability $(P)$ and Weight ($W$)}
\label{sec:app:snn:res:hc}

A second correlation we examined was that between coalition entropy ($H_C$), synaptic connection probability ($P$) and weight ($W$). Once more we obtained a result in line with \cite{Bhowmik2013}, with $H_C$ decreasing consistently with $P$ and slightly decreasing with $W$. It is worth noting that these results are more demarcated at $\gamma = 0.8$, with $\gamma = 0.95$ displaying less conclusive correlations, as can be observed in Figure \ref{fig:p_w_hc_all}.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_hc_8_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_hc_95_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
			Coalition Entropy ($H_C$) vs Connection Probability $(P)$ and Weight ($W$)
			\label{fig:p_w_hc_all}
		}
\end{figure}

\subsubsection{Metastability Index ($\lambda$) vs Synaptic Connection Probability $(P)$ and Weight ($W$)}
\label{sec:app:snn:res:lambda}
We then followed our analysis by focusing on the metastability index ($\lambda$), which we previously explained in Section \ref{sec:bg:lambda}. We plotted $\lambda$ against the synaptic connection probability ($P$) and weight ($W$), Once more, we observed a positive correlation between the measure and $P$, but no clear pattern between $\lambda$ and $W$, as shown in Figure \ref{fig:p_w_lambda_all}. These results are analogous to the ones between $\Psi$, $P$ and $W$, discussed in Section \ref{sec:app:snn:res:sync}.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_lambda_8_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_lambda_95_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
			Metastability Index ($\lambda$) vs Synaptic Connection Probability $(P)$ and Weight ($W$)
			\label{fig:p_w_lambda_all}
		}
\end{figure}

\subsubsection{Empirical Integrated Information ($\Phi_E$) vs Connection Probability $(P)$ and Weight ($W$)}
\label{sec:app:snn:res:phi-p-w}
Having examined correlations concerning global synchrony, metastability index and coalition entropy, we proceeded to examine how our implementations of empirical integrated information ($\Phi_E$) behaved vis-\`{a}-vis these measures and the model's parameters controlling weight ($W$) and synaptic connection probability ($P$). We first plotted $\Phi_E$ against $W$ and $P$ for both $\gamma = 0.8$ and $\gamma = 0.95$, as well as $\tau = 1$ and $\tau = 5$. The resulting surface plots are depicted in Figure \ref{fig:p_w_phi_all}. We observe that there are no clear correlations between the parameters and $\Phi_E$, as peaks and troughs occur throughout. It is worth underlining that for $\tau = 1$, the values of $\Phi_E$ are all negative, whereas the plots for $\tau = 5$ contain both negative and positive values.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_8_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_95_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_8_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_95_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information ($\Phi_E$) vs Connection Probability ($P$) and Weight ($W$).
		\label{fig:p_w_phi_all}
	}
\end{figure}

\subsubsection{Empirical Integrated Information ($\Phi_E$) vs Metastability Index ($\lambda$)}
\label{sec:app:snn:res:phi-lambda}
We continued our analysis by considering empirical integrated information ($\Phi_E$) and the metastability index ($\lambda$). As can be seen in Figure \ref{fig:phi_vs_lambda}, there is no clear relationship between both measures at any combination of $\gamma$ and $\tau$ value. Nevertheless, we observe a clear increase in $\Phi_E$ when moving from $\tau = 1$ to $\tau = 5$.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_lambda_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_lambda_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information ($\Phi_E$) vs Metastability Index ($\lambda$)
		\label{fig:phi_vs_lambda}
	}
\end{figure}

\subsubsection{Empirical Integrated Information ($\Phi_E$) vs Global Synchrony ($\Psi$)}
\label{sec:app:snn:res:phi-sync}
When examining empirical integrated information ($\Phi_E$) versus global synchrony ($\Psi$), we obtained results similar to those in Section \ref{sec:app:snn:res:phi-lambda}. Figure \ref{fig:phi_vs_psi} shows that there is no observable relationship between both measures at either $\gamma$ and either $\tau$ values. Once more, we can see an increase in $\Phi_E$ when moving from $\tau = 1$ to $\tau = 5$.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_psi_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_psi_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information ($\Phi_E$) vs Global Synchrony ($\Psi$)
		\label{fig:phi_vs_psi}
	}
\end{figure}

\subsubsection{Empirical Integrated Information ($\Phi_E$) vs Coalition Entropy ($H_C$)}
\label{sec:app:snn:res:phi-hc}
The analysis of empirical integrated information ($\Phi_E$) and $H_C$ showed a more conclusive positive correlation between both measures. Figure \ref{fig:phi_vs_hc} shows that both at $\gamma = 0.8$ and $\gamma = 0.95$ for both $\tau$ values, $\Phi_E$ increases with $H_C$. As with $\lambda$ and $\Psi$, we can also see an increase in $\Phi_E$ when moving from $\tau = 1$ to $\tau = 5$.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_hc_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_vs_hc_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information ($\Phi_E$) vs Coalition Entropy ($H_C$)
		\label{fig:phi_vs_hc}
	}
\end{figure}

% -----------------------------------------------
% Empirical Integrated Information Tilde Analysis
% -----------------------------------------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) Analysis}
\label{sec:app:snn:res:phi-tilde}
Having concluded our analysis using empirical integrated information ($\Phi_E$), we examined the same relationships using our alternative implementation, empirical integrated information tilde ($\widetilde{\Phi}_E$). As can be seen in Figures \ref{fig:p_w_phi_tilde_all} through \ref{fig:phi_tilde_vs_hc}, we observed the same results as using $\Phi_E$, with the clear the difference that $\widetilde{\Phi}_E$ does not take on negative values. Furthermore, although there appears to be a positive correlation between $\widetilde{\Phi}_E$ and coalition entropy ($H_C$) as shown in Figure \ref{fig:phi_tilde_vs_hc}, it is not as demarcated as the one between $\Phi_E$ and $H_C$ depicted in Figure \ref{fig:phi_vs_hc}.

% Empirical Integrated Information Tilde vs Probability and Weight
% ----------------------------------------------------------------
\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_tilde_8_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_tilde_95_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_tilde_8_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/p_w_phi_tilde_95_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Connection Probability ($P$) and Weight ($W$).
		\label{fig:p_w_phi_tilde_all}
	}
\end{figure}

% Empirical Integrated Information Tilde vs Metastability Index
% -------------------------------------------------------------
\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_lambda_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_lambda_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Metastability Index ($\lambda$).
		\label{fig:phi_tilde_vs_lambda}
	}
\end{figure}

% Empirical Integrated Information Tilde vs Global Synchrony
% ----------------------------------------------------------
\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_psi_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_psi_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Global Synchrony ($\Psi$).
		\label{fig:phi_tilde_vs_psi}
	}
\end{figure}

% Empirical Integrated Information Tilde vs Coalition Entropy
% -----------------------------------------------------------
\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_hc_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_hc_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Coalition Entropy ($H_C$).
		\label{fig:phi_tilde_vs_hc}
	}
\end{figure}

% --------------------------------------------------------------------------
% Empirical Integrated Information Tilde vs Empirical Integrated Information
% --------------------------------------------------------------------------
\subsubsection{Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Empirical Integrated Information ($\Phi_E$)}

Finally, we investigated the behaviour of $\widetilde{\Phi}_{E}$ against $\Phi_{E}$. As with our analysis using Kuramoto oscillators, we see a positive correlation and consistently higher values of $\widetilde{\Phi}_{E}$ than their respective $\Phi_E$ value.  We observe, however, that the shape of the plotted points is not parallel to the $\widetilde{\Phi}_{E} = \Phi_E$ line in either plot, with the one for $\tau = 1$ being significantly flatter. With $\tau = 5$, however, we also see a wider spread amongst $\widetilde{\Phi}_{E}$ for lower values of $\Phi_E$.

\begin{figure}[H] 
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_phi_1}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\begin{minipage}[b]{0.5\linewidth}
		\begin{center}
		\includegraphics[scale = 0.2]{figures/snn/phi_tilde_vs_phi_5}
		\end{center}
		\vspace{4ex}
	\end{minipage}
	\caption{
		Empirical Integrated Information Tilde ($\widetilde{\Phi}_E$) vs Empirical Integrated Information ($\Phi_E$).
		\label{fig:phi_tilde_vs_phi}
	}
\end{figure}

\subsection{Discussion}
\label{sec:snn:disc}

As evidenced in Section \ref{sec:snn:res}, our results did not provide conclusive insight into the behaviour of empirical integrated information in this system of spiking neural networks. We can first draw our attention to the behaviour of global synchrony ($\Psi$) and coalition entropy ($H_C$) against variations in weight ($W$) and synaptic connection probability ($P$) in our simulations compared to those in \cite{Bhowmik2013}. Although we observed a clear increase in $\Psi$ as $P$ increased, the positive correlation between $\Psi$ and $W$ seen in the original study was not present in our simulations. Similar results occur for coalition entropy $H_C$, where the negative correlation between $H_C$ and $P$ is visible, but that between $H_C$ and $W$, present in \cite{Bhowmik2013} is less evident in our study. This inconsistency leads us to believe that future work on the application of our implementation of $\Phi_E$ to populations of spiking neural networks will require a closer fine-tuning and adaptation of the simulations provided by \cite{Bhowmik2013}.

Generally speaking, our results of the behaviour of $\Phi_E$ and $\widetilde{\Phi}_E$ within this network are inconclusive. Figures \ref{fig:p_w_phi_all} through \ref{fig:phi_tilde_vs_phi} show little, if any correlation between the values for our $\Phi$ implementations, the model's parameters and the other measures of information. Only the relationship between $\Phi_E$ and coalition entropy $H_C$ (and the analogous one for $\widetilde{\Phi}_E$) shows a hint of consistency with the results found in Section \ref{MSUKO}. Furthermore, although there is some evidence of positive correlation between $\Phi_E$ and $\widetilde{\Phi}_E$, it is nowhere near as rigorous as the one seen in Figure \ref{fig:phi-tilde-vs-phi}. Once more, this accentuates the need for further examination of the relationship between both measures. 

It is also important to consider that the methodology to define the discrete time series data from the continuous output of the original model might need reconsideration. Given that the number of possible partitions of this network makes it impractical to perform exact computation, we approached the problem by using a heuristic approximation that led to the current methods and results. This adds complexity to our analysis as we need to consider whether the inconsistencies observed are due to our methods of approximation, or whether $\Phi$ itself behaves inconclusively for this type of systems. As discussed in \ref{sec:fw:snn}, future work will involve exploring more approximation strategies.

\clearpage

% =======================================================================================
% FUTURE WORK
% =======================================================================================
\section{Future Work}
\label{sec:fw}

\subsection{Ensure Robustness of $\Phi$ Implementation for JIDT}
\label{sec:fw:jidt}
One of the objectives of this project is to make the implementation of $\Phi$ for the JIDT robust enough that it is included in the official distribution. In order to do this, we aim to collaborate with Lizier to make conducting studies into integrated information using time series data more easily accessible for other researchers.

\subsubsection{Common Interface for $\Phi_E$ and $\widetilde{\Phi}_E$}
\label{sec:fw:jidt:interface}
For our project we implemented two version of empirical integrated information as presented in \cite{Barrett2011}. These, namely $\Phi_E$ and $\widetilde{\Phi}_E$, have their respective \textit{calculator} classes as explained in detail in Sections \ref{sec:impl:ii:calculator} and \ref{sec:impl:ii-tilde:calculator}. These classes share various methods and properties in common, which we believe could be factored out into a common interface or superclass that would make maintenance and further development more manageable. Moving forward, we would aim to achieve this by liaising with Lizier from the JIDT in order to structure it in the way that best fits the practices set forth by the JIDT.

\subsubsection{$\Phi$ Calculator Classes for Continuous Data}
\label{sec:fw:jidt:continuous}
Our study focused on discrete time series data, which we inputted into our simulation. Hence, our contribution to the JIDT only includes Java classes dealing with discrete data. In order to provide the flexibility needed by other researchers, we have as a future objective to implement the continuous versions of the classes presented in Section \ref{sec:impl}.


\subsubsection{Other Versions of $\Phi$}
\label{sec:fw:jidt:other-phi}
Currently we have two working implementations of $\Phi_{E}$. In order to provide the user with flexibility, we aim to extend our contribution to the JIDT to include other versions of integrated information such as $\Phi_{DM}$ and $\Phi_{AR}$ \cite{Barrett2011}. This will allow the user to easily test the discrepancies between the different measures. We would pair this extension with the interface proposed in Section \ref{sec:fw:jidt:interface} so as to ensure that the different versions of $\Phi$ are easily interchangeable were possible.

\subsubsection{Smoothing using Dirichlet Distribution}
\label{sec:fw:jidt:dirichlet}

The JIDT currently calculates mutual information for discrete data without taking into account the possibility that a state has not been observed given a relatively small input. This causes the probability of those unobserved states to be zero, potentially giving a skewed picture of the probability distribution. In order to correct for this, we could overload the JIDT's mutual information calculator for discrete data so that it can take an extra parameter $\alpha$. With this term we could then smooth the posterior distribution of the input data using a symmetric Dirichlet distribution with $\alpha$ as a prior.

\subsubsection{Testing}
\label{sec:fw:jidt:testing}
In order to ensure that our implementation follows good software engineering practices and aligns itself with the tests provided by JIDT, we aim to provide test suite to cover both common and edge cases of the classes we implemented for this project. This would be inline with the policies put forth by the JIDT and would allow us to seamlessly integrate our code with the JIDT. We propose that this be done using JUnit \cite{JUnit}, as this is the testing framework used by default by the JIDT.

\subsection{Expand Surrogate Data Analysis}
\label{sec:fw:surrogate}
While performing the surrogate data analysis we conducted for our implementations of empirical integrated information applied on populations of Kuramoto oscillators, we encountered several anomalies that were outside the scope of this study, but which we considered interesting areas for further investigation. We would thus propose that a more in-depth surrogate data analysis that could cover the points highlighted below would be a worthwhile extension for this project.

\begin{enumerate}
\item{Investigate negative stochastic interaction for sorted time series.}
\item{Further investigate increase in $\Phi_E$ and $\widetilde{\Phi}_E$ for shuffled time series.}
\item{Apply implementations of empirical integrated information to other generative models.}.
\item{Consider other normalisation factors other than Equations \ref{eq:norm} and \ref{eq:norm-tilde} for $\Phi_E$ and $\widetilde{\Phi}_E$ respectively, and analyse its impact on surrogate data.}
\end{enumerate}

\subsection{Application to Other Spiking Neural Network Architectures}
\label{sec:fw:snn}
We can also extend the application of our implementation of $\Phi$ to other models of spiking neural networks. A possible candidate is the one defined by Buehlmann and Deco in \cite{Buehlmann2010}. This would allow us to further investigate the relationship between integrated information ($\Phi$) and the specific properties of these architectures, aiding us in understanding whether $\Phi$ performs differently under different architectures or if it performs consistently. Additionally, it is important to consider different ways of analysing the output from these models, as computing exact values for the number of possible partitions and coalitions is computationally infeasible. Future work should focus on a rigorous methodology to tackle this problem of complexity.

\clearpage

% =======================================================================================
% CONCLUSION
% =======================================================================================
\section{Conclusion}
\label{sec:conclusion}

In completing this project, we achieved a number of our initial goals and found areas in which future studies could complement and further the rigour of the results found and the application of tools developed as part of our investigation.

Firstly, we were able to successfully implement a working version of empirical integrated information ($\Phi_E$) and its variation $\widetilde{\Phi}_E$ that is suitable for incorporation with the JIDT. As we make these tools available through the central repository for the JIDT or through a specific fork, we will allow the continued development of tested code that can be used to continue the study of integrated information, its behaviour and its properties. It will also be readily available for other researchers to use in their studies of $\Phi$ and its application to time series data.

Secondly we found that in the application of our implementation to populations of Kuramoto oscillators, $\Phi_E$ and $\widetilde{\Phi}_E$ behaved consistently and exhibited correlations with other information theoretic measures presented in \cite{Shanahan2010}. The relationships between $\Phi_E$ and measures related to metastability, such as the metastability index ($\lambda$) and the chimera index ($\chi$) provide evidence of a possible link between these that is worth considering for future investigation.

Thirdly, although our application to communities of PING oscillators gave inconclusive results, we propose that these could be related to the heuristic approximation methods we used to overcome the complexity of calculating subsystem partitions and coalitions. A more in-depth analysis of such methodology could provide useful intuition regarding the pros and cons of these heuristic approximations and lead to a rigorous method that could generate more conclusive results. Similarly, it is worth exploring the effect of the different parameters used in \cite{Bhowmik2013} on the nature of the networks generated, as we were unable to replicate certain correlations found in the original study.

Finally, unexpected results in our surrogate data analysis suggests that $\Phi_E$ could behave in unconventional ways under certain contexts, as evidenced in its increase when applied to shuffled time series data outputted by the populations of Kuramoto oscillators. A close analysis of this phenomena could also help solidify our results in Section \ref{sec:app:osc:res} and by extension the findings put forth in \cite{Shanahan2010} and \cite{Bhowmik2013}.

\clearpage

% =======================================================================================
% REFERENCES
% =======================================================================================
\bibliographystyle{plain}
\bibliography{report}{}
\clearpage

\end{document}