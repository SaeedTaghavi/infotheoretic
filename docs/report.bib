@article{Acebron2005,
abstract = {Synchronization phenomena in large populations of interacting elements are the subject of intense $\backslash$nresearch efforts in physical, biological, chemical, and social systems. A successful approach to the $\backslash$nproblem of synchronization consists of modeling each member of the population as a phase oscillator. $\backslash$nIn this review, synchronization is analyzed in one of the most representative models of coupled phase $\backslash$noscillators, the Kuramoto model.Arigorous mathematical treatment, specific numerical methods, and $\backslash$nmany variations and extensions of the original model that have appeared in the last few years are $\backslash$npresented. Relevant applications of the model in different contexts are also included.},
author = {Acebr\'{o}n, Juan and Bonilla, L. L. and Vicente, Conrad J P\'{e}rez and Ritort, F\'{e}lix and Spigler, Renato},
doi = {10.1103/RevModPhys.77.137},
file = {:Users/juancarlosfarah/Downloads/ACEBONPERRITSPI.RMP05.pdf:pdf},
isbn = {0034-6861},
issn = {00346861},
journal = {Reviews of Modern Physics},
number = {1},
pages = {137--185},
pmid = {229544100003},
title = {{The Kuramoto model: A simple paradigm for synchronization phenomena}},
volume = {77},
year = {2005}
}
@article{Arthuis2009,
abstract = {Loss of consciousness (LOC) is a dramatic clinical manifestation of temporal lobe seizures. Its underlying mechanism could involve altered coordinated neuronal activity between the brain regions that support conscious information processing. The consciousness access hypothesis assumes the existence of a global workspace in which information becomes available via synchronized activity within neuronal modules, often widely distributed throughout the brain. Re-entry loops and, in particular, thalamo-cortical communication would be crucial to functionally bind different modules together. In the present investigation, we used intracranial recordings of cortical and subcortical structures in 12 patients, with intractable temporal lobe epilepsy (TLE), as part of their presurgical evaluation to investigate the relationship between states of consciousness and neuronal activity within the brain. The synchronization of electroencephalography signals between distant regions was estimated as a function of time by using non-linear regression analysis. We report that LOC occurring during temporal lobe seizures is characterized by increased long-distance synchronization between structures that are critical in processing awareness, including thalamus (Th) and parietal cortices. The degree of LOC was found to correlate with the amount of synchronization in thalamo-cortical systems. We suggest that excessive synchronization overloads the structures involved in consciousness processing, preventing them from treating incoming information, thus resulting in LOC.},
author = {Arthuis, Marie and Valton, Luc and Rgis, Jean and Chauvel, Patrick and Wendling, Fabrice and Naccache, Lionel and Bernard, Christophe and Bartolomei, Fabrice},
doi = {10.1093/brain/awp086},
file = {:Users/juancarlosfarah/Downloads/2091.full.pdf:pdf},
isbn = {1460-2156 (Electronic)$\backslash$n0006-8950 (Linking)},
issn = {00068950},
journal = {Brain},
keywords = {Consciousness,EEG,Global workspace,Synchrony,Temporal lobe epilepsy},
number = {8},
pages = {2091--2101},
pmid = {19416952},
title = {{Impaired consciousness during temporal lobe seizures is related to increased long-distance corticalsubcortical synchronization}},
volume = {132},
year = {2009}
}
@article{Ay2015,
abstract = {Interdependencies of stochastically interacting units are usually$\backslash$nquantified by the Kullback-Leibler divergence of a stationary joint$\backslash$nprobability distribution on the set of all configurations from the$\backslash$ncorresponding factorized distribution. This is a spatial approach$\backslash$nwhich does not describe the intrinsically temporal aspects of interaction.$\backslash$nIn the present paper the setting is extended to a dynamical version$\backslash$nwhere temporal interdependencies are also captured by using information$\backslash$ngeometry of Markov-chain manifolds.},
author = {Ay, Nihat},
doi = {10.3390/e17042432},
file = {:Users/juancarlosfarah/Downloads/entropy-17-02432.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {complexity,information geometry,kullback-leibler divergence,markov chains,random fields,separability,stochastic interaction},
month = apr,
number = {4},
pages = {2432--2458},
title = {{Information Geometry on Complexity and Stochastic Interaction}},
url = {http://www.mis.mpg.de/publications/preprints/2001/prepr2001-95.html http://www.mdpi.com/1099-4300/17/4/2432/},
volume = {17},
year = {2015}
}
@article{Balduzzi2008,
abstract = {This paper introduces a time- and state-dependent measure of integrated information, phi, which captures the repertoire of causal states available to a system as a whole. Specifically, phi quantifies how much information is generated (uncertainty is reduced) when a system enters a particular state through causal interactions among its elements, above and beyond the information generated independently by its parts. Such mathematical characterization is motivated by the observation that integrated information captures two key phenomenological properties of consciousness: (i) there is a large repertoire of conscious experiences so that, when one particular experience occurs, it generates a large amount of information by ruling out all the others; and (ii) this information is integrated, in that each experience appears as a whole that cannot be decomposed into independent parts. This paper extends previous work on stationary systems and applies integrated information to discrete networks as a function of their dynamics and causal architecture. An analysis of basic examples indicates the following: (i) phi varies depending on the state entered by a network, being higher if active and inactive elements are balanced and lower if the network is inactive or hyperactive. (ii) phi varies for systems with identical or similar surface dynamics depending on the underlying causal architecture, being low for systems that merely copy or replay activity states. (iii) phi varies as a function of network architecture. High phi values can be obtained by architectures that conjoin functional specialization with functional integration. Strictly modular and homogeneous systems cannot generate high phi because the former lack integration, whereas the latter lack information. Feedforward and lattice architectures are capable of generating high phi but are inefficient. (iv) In Hopfield networks, phi is low for attractor states and neutral states, but increases if the networks are optimized to achieve tension between local and global interactions. These basic examples appear to match well against neurobiological evidence concerning the neural substrates of consciousness. More generally, phi appears to be a useful metric to characterize the capacity of any physical system to integrate information.},
author = {Balduzzi, David and Tononi, Giulio},
doi = {10.1371/journal.pcbi.1000091},
file = {:Users/juancarlosfarah/Mendeley Library/Balduzzi, Tononi/Balduzzi, Tononi - 2008 - Integrated information in discrete dynamical systems Motivation and theoretical framework.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {6},
pmid = {18551165},
title = {{Integrated information in discrete dynamical systems: Motivation and theoretical framework}},
volume = {4},
year = {2008}
}
@article{Barrett2011,
abstract = {A recent measure of ‘integrated information’, $\phi$DM, quantifies the extent to which a system generates more information than the sum of its parts as it transitions between states, possibly reflecting levels of consciousness generated by neural systems. However, $\phi$DM is defined only for discrete Markov systems, which are unusual in biology; as a result, $\phi$DM can rarely be measured in practice. Here, we describe two new measures, $\phi$E and $\phi$AR, that overcome these limitations and are easy to apply to time-series data. We use simulations to demonstrate the in-practice applicability of our measures, and to explore their properties. Our results provide new opportunities for examining information integration in real and model systems and carry implications for relations between integrated information, consciousness, and other neurocognitive processes. However, our findings pose challenges for theories that ascribe physical meaning to the measured quantities.},
author = {Barrett, Adam B. and Seth, Anil K.},
doi = {10.1371/journal.pcbi.1001052},
file = {:Users/juancarlosfarah/Mendeley Library/Barrett, Seth/Barrett, Seth - 2011 - Practical Measures of Integrated Information for Time-Series Data(2).pdf:pdf;:Users/juancarlosfarah/Mendeley Library/Barrett, Seth/Barrett, Seth - 2011 - Practical measures of integrated information for time-series data.pdf:pdf},
isbn = {0738202967},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pmid = {21283779},
title = {{Practical Measures of Integrated Information for Time-Series Data}},
volume = {7},
year = {2011}
}
@article{Bhowmik2013,
abstract = {Groups of neurons firing synchronously are hypothesized to underlie many cognitive functions such as attention, associative learning, memory, and sensory selection. Recent theories suggest that transient periods of synchronization and desynchronization provide a mechanism for dynamically integrating and forming coalitions of functionally related neural areas, and that at these times conditions are optimal for information transfer. Oscillating neural populations display a great amount of spectral complexity, with several rhythms temporally coexisting in different structures and interacting with each other. This paper explores inter-band frequency modulation between neural oscillators using models of quadratic integrate-and-fire neurons and Hodgkin-Huxley neurons. We vary the structural connectivity in a network of neural oscillators, assess the spectral complexity, and correlate the inter-band frequency modulation. We contrast this correlation against measures of metastable coalition entropy and synchrony. Our results show that oscillations in different neural populations modulate each other so as to change frequency, and that the interaction of these fluctuating frequencies in the network as a whole is able to drive different neural populations towards episodes of synchrony. Further to this, we locate an area in the connectivity space in which the system directs itself in this way so as to explore a large repertoire of synchronous coalitions. We suggest that such dynamics facilitate versatile exploration, integration, and communication between functionally related neural areas, and thereby supports sophisticated cognitive processing in the brain.},
author = {Bhowmik, David and Shanahan, Murray},
doi = {10.1371/journal.pone.0062234},
file = {:Users/juancarlosfarah/Downloads/Metastability2.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {23614040},
title = {{Metastability and Inter-Band Frequency Modulation in Networks of Oscillating Spiking Neuron Populations}},
volume = {8},
year = {2013}
}
@article{Buehlmann2010,
abstract = {In recent experimental work it has been shown that neuronal interactions are modulated by neuronal synchronization and that this modulation depends on phase shifts in neuronal oscillations. This result suggests that connections in a network can be shaped through synchronization. Here, we test and expand this hypothesis using a model network. We use transfer entropy, an information theoretical measure, to quantify the exchanged information. We show that transferred information depends on the phase relation of the signal, that the amount of exchanged information increases as a function of oscillations in the signal and that the speed of the information transfer increases as a function of synchronization. This implies that synchronization makes information transport more efficient. In summary, our results reinforce the hypothesis that synchronization modulates neuronal interactions and provide further evidence that gamma band synchronization has behavioral relevance.},
author = {Buehlmann, Andres and Deco, Gustavo},
doi = {10.1371/journal.pcbi.1000934},
file = {:Users/juancarlosfarah/Downloads/Metastability.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {9},
pmid = {20862355},
title = {{Optimal information transfer in the cortex through synchronization}},
volume = {6},
year = {2010}
}
@book{Burnham2002,
author = {Burnham, K. P. and Anderson, D. R.},
pages = {51},
publisher = {Springer},
title = {{Model Selection and Multi-Model Inference}},
year = {2002}
}
@article{Cumin2007,
abstract = {In this article, we have generalised the Kuramoto model to allow one to model neuronal synchronisation more appropriately. The generalised version allows for different connective arrangements, time-varying natural frequencies and time-varying coupling strengths to be realised within the framework of the original Kuramoto model. By incorporating the above mentioned features into the original Kuramoto model one can allow for the adaptive nature of neurons in the brain to be accommodated. Extensive tests using the generalised Kuramoto model were performed on a N = 4 coupled oscillator network. Examination of how different connective arrangements, time-varying natural frequencies and time-varying coupling strengths affected synchronisation separately and in combination are reported. The effects on synchronisation for large N are also reported. © 2006 Elsevier Ltd. All rights reserved.},
author = {Cumin, D. and Unsworth, C. P.},
doi = {10.1016/j.physd.2006.12.004},
file = {:Users/juancarlosfarah/Downloads/1-s2.0-S0167278906004805-main.pdf:pdf},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
keywords = {Brain,Coupled oscillators,Kuramoto model,Lattice,Neuron,Synchronisation},
number = {2},
pages = {181--196},
title = {{Generalising the Kuramoto model for the study of neuronal synchronisation in the brain}},
volume = {226},
year = {2007}
}
@article{Durstenfeld1964,
author = {Durstenfeld, Richard},
doi = {10.1145/364520.364540},
file = {:Users/juancarlosfarah/Downloads/p420-forsythe.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = jul,
number = {7},
pages = {420},
title = {{Algorithm 235: Random permutation}},
url = {http://portal.acm.org/citation.cfm?doid=364520.364540},
volume = {7},
year = {1964}
}
@article{Eaton,
author = {Eaton, John W.},
journal = {https://www.gnu.org/software/octave/},
title = {{GNU Octave}},
url = {https://www.gnu.org/software/octave/}
}
@article{GitHub,
author = {GitHub},
journal = {https://github.com/about},
title = {{About GitHub}},
url = {https://github.com/about}
}
@article{Hodgkin1952,
abstract = {This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424-448; Hodgkin and Huxley 1952,J. Physiol.116, 449-566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3-6).},
author = {Hodgkin, L. and Huxley, F.},
doi = {10.1016/S0092-8240(05)80004-7},
file = {:Users/juancarlosfarah/Downloads/jphysiol01442-0106.pdf:pdf},
isbn = {1546-1726 (Electronic)$\backslash$n1097-6256 (Linking)},
issn = {00928240},
journal = {Bulletin of Mathematical Biology},
pages = {500--544},
pmid = {2185861},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
volume = {117},
year = {1952}
}
@article{Hudson2014,
abstract = {It is not clear how, after a large perturbation, the brain explores the vast space of potential neuronal activity states to recover those compatible with consciousness. Here, we analyze recovery from pharmacologically induced coma to show that neuronal activity en route to consciousness is confined to a low-dimensional subspace. In this subspace, neuronal activity forms discrete metastable states persistent on the scale of minutes. The network of transitions that links these metastable states is structured such that some states form hubs that connect groups of otherwise disconnected states. Although many paths through the network are possible, to ultimately enter the activity state compatible with consciousness, the brain must first pass through these hubs in an orderly fashion. This organization of metastable states, along with dramatic dimensionality reduction, significantly simplifies the task of sampling the parameter space to recover the state consistent with wakefulness on a physiologically relevant timescale.},
author = {Hudson, Andrew E and Calderon, Diany Paola and Pfaff, Donald W and Proekt, Alex},
doi = {10.1073/pnas.1408296111},
file = {:Users/juancarlosfarah/Downloads/PNAS-2014-Hudson-9283-8.pdf:pdf},
isbn = {0027-8424},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences},
keywords = {Anesthesia,Consciousness,Consciousness: anesthesia},
number = {25},
pages = {9283--8},
pmid = {24927558},
title = {{Recovery of consciousness is mediated by a network of discrete metastable activity states.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4078822\&tool=pmcentrez\&rendertype=abstract$\backslash$nhttp://www.scientificamerican.com/podcast/episode/brain-state-bread-crumbs-lead-way-back-to-consciousness/?\&WT.mc\_id=SA\_DD\_20140729},
volume = {111},
year = {2014}
}
@article{JUnit,
author = {JUnit},
journal = {http://junit.org/},
title = {{About JUnit}}
}
@misc{Kelso2012,
abstract = {Multistable coordination dynamics exists at many levels, from multifunctional neural circuits in vertebrates and invertebrates to large-scale neural circuitry in humans. Moreover, multistability spans (at least) the domains of action and perception, and has been found to place constraints upon, even dictating the nature of, intentional change and the skill-learning process. This paper reviews some of the key evidence for multistability in the aforementioned areas, and illustrates how it has been measured, modelled and theoretically understood. It then suggests how multistability--when combined with essential aspects of coordination dynamics such as instability, transitions and (especially) metastability--provides a platform for understanding coupling and the creative dynamics of complex goal-directed systems, including the brain and the brain-behaviour relation.},
author = {Kelso, J. S.},
booktitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
doi = {10.1098/rstb.2011.0351},
file = {:Users/juancarlosfarah/Downloads/906.full.pdf:pdf},
isbn = {0962-8436},
issn = {0962-8436},
keywords = {coordination dynamics,instability,metastability,multistability,transitions},
number = {1591},
pages = {906--918},
pmid = {22371613},
title = {{Multistability and metastability: understanding dynamic coordination in the brain}},
volume = {367},
year = {2012}
}
@article{Kheradpisheh2015,
abstract = {Retinal image of surrounding objects varies tremendously due to the changes in position, size, pose, illumination condition, background context, occlusion, noise, and nonrigid deformations. But despite these huge variations, our visual system is able to invariantly recognize any object in just a fraction of a second. To date, various computational models have been proposed to mimic the hierarchical processing of the ventral visual pathway, with limited success. Here, we show that combining a biologically inspired network architecture with a biologically inspired learning rule significantly improves the models' performance when facing challenging object recognition problems. Our model is an asynchronous feedforward spiking neural network. When the network is presented with natural images, the neurons in the entry layers detect edges, and the most activated ones fire first, while neurons in higher layers are equipped with spike timing-dependent plasticity. These neurons progressively become selective to intermediate complexity visual features appropriate for object categorization, as demonstrated using the 3D Object dataset provided by Savarese et al. at CVGLab, Stanford University. The model reached 96\% categorization accuracy, which corresponds to two to three times fewer errors than the previous state-of-the-art, demonstrating that it is able to accurately recognize different instances of multiple object classes in various appearance conditions (different views, scales, tilts, and backgrounds). Several statistical analysis techniques are used to show that our model extracts class specific and highly informative features.},
archivePrefix = {arXiv},
arxivId = {1504.03871},
author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Masquelier, Timoth\'{e}e},
eprint = {1504.03871},
file = {:Users/juancarlosfarah/Downloads/1504.03871v2.pdf:pdf},
journal = {arXiv preprint arXiv: \ldots},
keywords = {spiking neurons,stdp,temporal,view-invariant object recognition,visual cortex},
title = {{Bio-inspired Unsupervised Learning of Visual Features Leads to Robust Invariant Object Recognition}},
url = {http://arxiv.org/abs/1504.03871},
year = {2015}
}
@article{Lizier2014,
abstract = {Complex systems are increasingly being viewed as distributed information processing systems, particularly in the domains of computational neuroscience, bioinformatics and Artificial Life. This trend has resulted in a strong uptake in the use of (Shannon) information-theoretic measures to analyse the dynamics of complex systems in these fields. We introduce the Java Information Dynamics Toolkit (JIDT): a Google code project which provides a standalone, (GNU GPL v3 licensed) open-source code implementation for empirical estimation of information-theoretic measures from time-series data. While the toolkit provides classic information-theoretic measures (e.g. entropy, mutual information, conditional mutual information), it ultimately focusses on implementing higher-level measures for information dynamics. That is, JIDT focusses on quantifying information storage, transfer and modification, and the dynamics of these operations in space and time. For this purpose, it includes implementations of the transfer entropy and active information storage, their multivariate extensions and local or pointwise variants. JIDT provides implementations for both discrete and continuous-valued data for each measure, including various types of estimator for continuous data (e.g. Gaussian, box-kernel and Kraskov-Stoegbauer-Grassberger) which can be swapped at run-time due to Java's object-oriented polymorphism. Furthermore, while written in Java, the toolkit can be used directly in MATLAB, GNU Octave and Python. We present the principles behind the code design, and provide several examples to guide users.},
archivePrefix = {arXiv},
arxivId = {1408.3270},
author = {Lizier, Joseph T},
doi = {10.1093/bioinformatics/btu612},
eprint = {1408.3270},
file = {:Users/juancarlosfarah/Git/infodynamics/InfoDynamicsToolkit.pdf:pdf},
issn = {2296-9144},
keywords = {complex networks,information,information storage,information transfer,intrinsic computation,java,matlab,octave,python,theory,transfer entropy},
pages = {33},
title = {{JIDT: An information-theoretic toolkit for studying the dynamics of complex systems}},
url = {http://arxiv.org/abs/1408.3270},
year = {2014}
}
@article{MongoDB,
author = {MongoDB},
journal = {http://docs.mongodb.org/manual/},
title = {{The MongoDB 3.0 Manual}},
url = {http://docs.mongodb.org/manual/}
}
@article{Schreiber2000,
abstract = {An information theoretic measure is derived that quantifies the statistical coherence between systems evolving in time. The standard time delayed mutual information fails to distinguish information that is actually exchanged from shared information due to common history and input signals. In our new approach, these influences are excluded by appropriate conditioning of transition probabilities. The resulting transfer entropy is able to distinguish effectively driving and responding elements and to detect asymmetry in the interaction of subsystems.},
author = {Schreiber, T},
doi = {10.1103/PhysRevLett.85.461},
file = {:Users/juancarlosfarah/Downloads/PhysRevLett.85.461.pdf:pdf},
isbn = {0031-9007},
issn = {1079-7114},
journal = {Physical review letters},
number = {2},
pages = {461--4},
pmid = {10991308},
title = {{Measuring information transfer}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10991308},
volume = {85},
year = {2000}
}
@incollection{Seth2009,
abstract = {Conventional approaches to understanding consciousness are generally concerned with the contribution of specific brain areas or groups of neurons. By contrast, it is considered here what kinds of neural processes can account for key properties of conscious experience. Applying measures of neural integration and complexity, together with an analysis of extensive neurological data, leads to a testable proposal-the dynamic core hypothesis-about the properties of the neural substrate of consciousness.},
author = {Seth, Anil K and Edelman, Gerald M},
booktitle = {Encyclopedia of Complexity and Systems Science},
doi = {10.1007/978-0-387-30440-3\_94},
file = {:Users/juancarlosfarah/Downloads/SethEdelmanSpringer2008.pdf:pdf},
isbn = {978-0-387-75888-6},
number = {5395},
pages = {1424--1443},
publisher = {Springer New York},
title = {{Consciousness and Complexity}},
url = {http://link.springer.com/10.1007/978-0-387-30440-3\_94},
volume = {282},
year = {2009}
}
@article{Shanahan2010,
abstract = {A system of symmetrically coupled identical oscillators with phase lag is presented, which is capable of generating a large repertoire of transient (metastable) "chimera" states in which synchronization and desynchronization coexist. The oscillators are organized into communities, such that each oscillator is connected to all its peers in the same community and to a subset of the oscillators in other communities. Measures are introduced for quantifying metastability, the prevalence of chimera states, and the variety of such states a system generates. By simulation, it is shown that each of these measures is maximized when the phase lag of the model is close, but not equal, to pi/2. The relevance of the model to a number of fields is briefly discussed with particular emphasis on brain dynamics.},
archivePrefix = {arXiv},
arxivId = {0908.3881},
author = {Shanahan, Murray},
doi = {10.1063/1.3305451},
eprint = {0908.3881},
file = {:Users/juancarlosfarah/Downloads/0908.3881v3.pdf:pdf},
issn = {10541500},
journal = {Chaos},
number = {1},
pages = {1--18},
pmid = {20370263},
title = {{Metastable chimera states in community-structured oscillator networks}},
volume = {20},
year = {2010}
}
@article{Silvester,
author = {Silvester, Steven},
journal = {http://blink1073.github.io/oct2py/},
title = {{Oct2Py: Python to GNU Octave Bridge}},
url = {http://blink1073.github.io/oct2py/}
}
@article{Strogatz2000,
abstract = {The Kuramoto model describes a large population of coupled limit-cycle oscillators whose natural frequencies are drawn $\backslash$nfrom some prescribed distribution. If the coupling strength exceeds a certain threshold, the system exhibits a phase transition: $\backslash$nsome of the oscillators spontaneously synchronize, while others remain incoherent. The mathematical analysis of this bifur- $\backslash$ncation has proved both problematic and fascinating. We review 25 years of research on the Kuramoto model, highlighting $\backslash$nthe false turns as well as the successes, but mainly following the trail leading from Kuramoto’s work to Crawford’s recent $\backslash$ncontributions. It is a lovely winding road, with excursions through mathematical biology, statistical physics, kinetic theory, $\backslash$nbifurcation theory, and plasma physics.},
author = {Strogatz, Steven H.},
doi = {10.1016/S0167-2789(00)00094-4},
file = {:Users/juancarlosfarah/Downloads/2000-01-Strogatz.pdf:pdf},
isbn = {978-0-333-78676-5},
issn = {01672789},
journal = {Physica D: Nonlinear Phenomena},
keywords = {coupled oscillators,kinetic theory,kuramoto model,plasma physics},
number = {1-4},
pages = {1--20},
title = {{From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators}},
volume = {143},
year = {2000}
}
@article{TheApacheSoftwareFoundation,
author = {{The Apache Software Foundation}},
journal = {https://maven.apache.org/what-is-maven.html},
title = {{What is Maven?}},
url = {https://maven.apache.org/what-is-maven.html}
}
@article{Tononi2008a,
abstract = {The integrated information theory (IIT) starts from phenomenology and makes use of thought experiments to claim that consciousness is integrated information. Specifically: (i) the quantity of consciousness corresponds to the amount of integrated information generated by a complex of elements; (ii) the quality of experience is specified by the set of informational relationships generated within that complex. Integrated information (Phi) is defined as the amount of information generated by a complex of elements, above and beyond the information generated by its parts. Qualia space (Q) is a space where each axis represents a possible state of the complex, each point is a probability distribution of its states, and arrows between points represent the informational relationships among its elements generated by causal mechanisms (connections). Together, the set of informational relationships within a complex constitute a shape in Q that completely and univocally specifies a particular experience. Several observations concerning the neural substrate of consciousness fall naturally into place within the IIT framework. Among them are the association of consciousness with certain neural systems rather than with others; the fact that neural processes underlying consciousness can influence or be influenced by neural processes that remain unconscious; the reduction of consciousness during dreamless sleep and generalized seizures; and the distinct role of different cortical architectures in affecting the quality of experience. Equating consciousness with integrated information carries several implications for our view of nature.},
author = {Tononi, Giulio},
doi = {215/3/216 [pii]},
file = {:Users/juancarlosfarah/Mendeley Library/Tononi/Tononi - 2008 - Consciousness as integrated information A provisional manifesto.pdf:pdf},
isbn = {0006-3185 (Print) 0006-3185 (Linking)},
issn = {00063185},
journal = {Biological Bulletin},
number = {3},
pages = {216--242},
pmid = {19098144},
title = {{Consciousness as integrated information: A provisional manifesto}},
volume = {215},
year = {2008}
}
@article{Tononi2004,
abstract = {BACKGROUND: Consciousness poses two main problems. The first is understanding the conditions that determine to what extent a system has conscious experience. For instance, why is our consciousness generated by certain parts of our brain, such as the thalamocortical system, and not by other parts, such as the cerebellum? And why are we conscious during wakefulness and much less so during dreamless sleep? The second problem is understanding the conditions that determine what kind of consciousness a system has. For example, why do specific parts of the brain contribute specific qualities to our conscious experience, such as vision and audition? PRESENTATION OF THE HYPOTHESIS: This paper presents a theory about what consciousness is and how it can be measured. According to the theory, consciousness corresponds to the capacity of a system to integrate information. This claim is motivated by two key phenomenological properties of consciousness: differentiation - the availability of a very large number of conscious experiences; and integration - the unity of each such experience. The theory states that the quantity of consciousness available to a system can be measured as the Phi value of a complex of elements. Phi is the amount of causally effective information that can be integrated across the informational weakest link of a subset of elements. A complex is a subset of elements with Phi>0 that is not part of a subset of higher Phi. The theory also claims that the quality of consciousness is determined by the informational relationships among the elements of a complex, which are specified by the values of effective information among them. Finally, each particular conscious experience is specified by the value, at any given time, of the variables mediating informational interactions among the elements of a complex. TESTING THE HYPOTHESIS: The information integration theory accounts, in a principled manner, for several neurobiological observations concerning consciousness. As shown here, these include the association of consciousness with certain neural systems rather than with others; the fact that neural processes underlying consciousness can influence or be influenced by neural processes that remain unconscious; the reduction of consciousness during dreamless sleep and generalized seizures; and the time requirements on neural interactions that support consciousness. IMPLICATIONS OF THE HYPOTHESIS: The theory entails that consciousness is a fundamental quantity, that it is graded, that it is present in infants and animals, and that it should be possible to build conscious artifacts.},
author = {Tononi, Giulio},
doi = {10.1186/1471-2202-5-42},
file = {:Users/juancarlosfarah/Mendeley Library/Tononi/Tononi - 2004 - An information integration theory of consciousness.pdf:pdf},
isbn = {14712202},
issn = {1471-2202},
journal = {BMC neuroscience},
pages = {42},
pmid = {15522121},
title = {{An information integration theory of consciousness.}},
volume = {5},
year = {2004}
}
@article{Tononi2003,
abstract = {BACKGROUND: To understand the functioning of distributed networks such as the brain, it is important to characterize their ability to integrate information. The paper considers a measure based on effective information, a quantity capturing all causal interactions that can occur between two parts of a system. RESULTS: The capacity to integrate information, or Phi, is given by the minimum amount of effective information that can be exchanged between two complementary parts of a subset. It is shown that this measure can be used to identify the subsets of a system that can integrate information, or complexes. The analysis is applied to idealized neural systems that differ in the organization of their connections. The results indicate that Phi is maximized by having each element develop a different connection pattern with the rest of the complex (functional specialization) while ensuring that a large amount of information can be exchanged across any bipartition of the network (functional integration). CONCLUSION: Based on this analysis, the connectional organization of certain neural architectures, such as the thalamocortical system, are well suited to information integration, while that of others, such as the cerebellum, are not, with significant functional consequences. The proposed analysis of information integration should be applicable to other systems and networks.},
author = {Tononi, Giulio and Sporns, Olaf},
doi = {10.1186/1471-2202-4-31},
file = {:Users/juancarlosfarah/Mendeley Library/Tononi, Sporns/Tononi, Sporns - 2003 - Measuring information integration.pdf:pdf},
isbn = {1471-2202},
issn = {1471-2202},
journal = {BMC neuroscience},
pages = {31},
pmid = {14641936},
title = {{Measuring information integration.}},
volume = {4},
year = {2003}
}
@misc{WikimediaFoundation2015,
author = {{Wikimedia Foundation}},
booktitle = {Wikipedia},
title = {{Integrated information theory}},
url = {https://en.wikipedia.org/wiki/Integrated\_information\_theory},
year = {2015}
}
@article{Bennett2004,
abstract = {Certain neurons in the mammalian brain have long been known to be joined by gap junctions, which are the most common type of electrical synapse. More recently, cloning of neuron-specific connexins, increased capability of visualizing cells within brain tissue, labeling of cell types by transgenic methods, and generation of connexin knockouts have spurred a rapid increase in our knowledge of the role of gap junctions in neural activity. This article reviews the many subtleties of transmission mediated by gap junctions and the mechanisms whereby these junctions contribute to synchronous firing.},
author = {Bennett, Michael V L and Zukin, R. Suzanne},
doi = {10.1016/S0896-6273(04)00043-1},
file = {:Users/juancarlosfarah/Downloads/1-s2.0-S0896627304000431-main.pdf:pdf},
isbn = {0896-6273 (Print)$\backslash$r0896-6273},
issn = {08966273},
journal = {Neuron},
number = {4},
pages = {495--511},
pmid = {14980200},
title = {{Electrical Coupling and Neuronal Synchronization in the Mammalian Brain}},
volume = {41},
year = {2004}
}
@book{Westra1999,
author = {Westra, Jan R. and Verhoeven, Chris J.M. and van Roermund, Arthur},
isbn = {9780792386520},
pages = {6--7},
publisher = {Springer Science \& Business Media},
title = {{Oscillators and Oscillator Systems: Classification, Analysis and Synthesis}},
year = {1999}
}
@book{Wolfs2004,
author = {Wolfs, Frank L. H.},
publisher = {University of Rochester},
title = {{Physics 235 Lecture Notes}},
year = {2004}
}
@misc{Abarbanel1996,
author = {Abarbanel, H.D. and Rabinovich, M.I. and Selverston, a. and Bazhenov, M.V. and Huerta, R. and Sushchik, M.M. and Rubchinskii, L.L.},
booktitle = {Uspekhi Fizicheskih Nauk},
doi = {10.3367/UFNr.0166.199604b.0363},
file = {:Users/juancarlosfarah/Downloads/Physics-Uspekhi1996.pdf:pdf},
issn = {0042-1294},
number = {4},
pages = {363},
title = {{Synchronisation in neural networks}},
volume = {166},
year = {1996}
}
